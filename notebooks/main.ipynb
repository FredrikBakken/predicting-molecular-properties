{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook initialized execution at 08.14.2019_11.18.47.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import pytz\n",
    "import operator\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import callbacks\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.losses import mean_absolute_error\n",
    "from tensorflow.python.keras.layers import Dense, Input, Activation\n",
    "from tensorflow.python.keras.layers import BatchNormalization, Add, Dropout\n",
    "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.python.keras.optimizers import Adam, Adadelta, SGD\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action = 'ignore', category = FutureWarning)\n",
    "warnings.filterwarnings(action = 'ignore', category = DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.check_repository import *\n",
    "from utils.generate_features import *\n",
    "\n",
    "time = datetime.now(pytz.timezone('Europe/Oslo')).strftime('%m.%d.%Y_%H.%M.%S')\n",
    "print(f'Notebook initialized execution at {time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_repository()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_optimization(dfs):\n",
    "    for df in dfs:\n",
    "        del df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df_train):\n",
    "    train_X, validation_X = train_test_split(df_train, test_size = 0.1, random_state = 0)\n",
    "\n",
    "    train_X = train_X.reset_index()\n",
    "    validation_X = validation_X.reset_index()\n",
    "\n",
    "    train_y = train_X['scalar_coupling_constant']\n",
    "    train_y = train_y.replace([np.inf, -np.inf], np.nan)\n",
    "    train_y = train_y.reset_index()\n",
    "    train_y = train_y.drop(['index'], axis = 1)\n",
    "    validation_y = validation_X['scalar_coupling_constant']\n",
    "    validation_y = validation_y.replace([np.inf, -np.inf], np.nan)\n",
    "    validation_y = validation_y.reset_index()\n",
    "    validation_y = validation_y.drop(['index'], axis = 1)\n",
    "\n",
    "    train_X = train_X.drop('scalar_coupling_constant', axis = 1)\n",
    "    validation_X = validation_X.drop('scalar_coupling_constant', axis = 1)\n",
    "    \n",
    "    train_X = train_X.drop(['index'], axis = 1)\n",
    "    validation_X = validation_X.drop(['index'], axis = 1)\n",
    "    \n",
    "    return train_X, train_y, validation_X, validation_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    # input layer\n",
    "    inp = Input(shape = (input_shape,))\n",
    "\n",
    "    # first hidden layer\n",
    "    x = Dense(256, kernel_initializer = 'he_normal')(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # second hidden layer\n",
    "    x = Dense(512, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # third hidden layer\n",
    "    x = Dense(1024, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # fourth hidden layer\n",
    "    x = Dense(512, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # fifth hidden layer\n",
    "    x = Dense(256, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # sixth hidden layer\n",
    "    x = Dense(128, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # seventh hidden layer\n",
    "    x = Dense(128, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # eight hidden layer\n",
    "    x = Dense(64, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # output layer scalar_coupling_constant\n",
    "    out = Dense(1, activation = 'linear')(x)   \n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(coupling_type, train_X, train_y, validation_X, validation_y):\n",
    "    epoch_n = 2000\n",
    "    verbose = 1\n",
    "    batch_size = 2048\n",
    "    model_name = f'../models/nn/coupling_model_{coupling_type}_NN.hdf5'\n",
    "    \n",
    "    nn_model = create_nn_model(train_X.shape[1])\n",
    "\n",
    "    nn_model.compile(loss = 'mae', optimizer = Adam())\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor = 'loss', min_delta = 0.00001, patience = 64,\n",
    "                                 verbose = verbose, mode = 'auto', restore_best_weights = True)\n",
    "    \n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 32,\n",
    "                                      min_lr = 1e-7, mode = 'auto', verbose = verbose)\n",
    "    \n",
    "    sv_mod = callbacks.ModelCheckpoint(model_name, monitor = 'val_loss', save_best_only = True,\n",
    "                                       save_weights_only = True, restore_best_weights = True)\n",
    "    \n",
    "    history = nn_model.fit(x = train_X.values, y = train_y['scalar_coupling_constant'].values, \n",
    "                           validation_data = (validation_X.values, validation_y['scalar_coupling_constant'].values),\n",
    "                           callbacks = [es, rlr, sv_mod], epochs = epoch_n, batch_size = batch_size, verbose = verbose)\n",
    "    \n",
    "    nn_model.save_weights(model_name)\n",
    "\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_train(coupling_type, train_X, train_y, validation_X, validation_y):\n",
    "    model_name_wrt = f'../models/xgb/coupling_model_{coupling_type}_XGB.hdf5'\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(base_score = 0.5, booster = 'gbtree', colsample_bylevel = 1,\n",
    "                                 colsample_bytree = 1, gamma = 0, importance_type = 'gain',\n",
    "                                 learning_rate = 0.1, max_delta_step = 0, max_depth = 9,\n",
    "                                 min_child_weight = 1, missing = None, n_estimators = 10000, n_jobs = -1,\n",
    "                                 nthread = None, objective = 'reg:squarederror', random_state = 101, reg_alpha = 2,\n",
    "                                 reg_lambda = 0.2, scale_pos_weight = 1, seed = None, silent = False, subsample = 1)\n",
    "\n",
    "    xgb_model.fit(train_X, train_y, eval_set = [(validation_X, validation_y)], eval_metric = 'mae', \n",
    "                  early_stopping_rounds = 32, verbose = True)   \n",
    "    \n",
    "    xgb_model.save_model(model_name_wrt)\n",
    "    #joblib.dump(xgb_model, model_name_wrt)\n",
    "    \n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance(xgb_model, train_X):\n",
    "    input_features = train_X.columns.values\n",
    "    feat_imp = xgb_model.feature_importances_\n",
    "    np.split(feat_imp, len(input_features))\n",
    "    \n",
    "    feat_imp_dict = {}\n",
    "    for i in range(0, len(input_features)):\n",
    "        feat_imp_dict[feat_imp[i]] = input_features[i]\n",
    "\n",
    "    sorted_feats = sorted(feat_imp_dict.items(), key = operator.itemgetter(0))\n",
    "    for i in range(len(sorted_feats) - 1, 0, -1):\n",
    "        print(sorted_feats[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Blending and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_blending_list(nn_val_predict, xgb_val_predict, validation_y):\n",
    "    nn_val_pred_error  = (validation_y - nn_val_predict)\n",
    "    xgb_val_pred_error = (validation_y - xgb_val_predict)\n",
    "    \n",
    "    val_error_corr = np.corrcoef(nn_val_pred_error, xgb_val_pred_error)\n",
    "    print(f'Error correlation: {val_error_corr[0][1]}. Error difference {xgb_accuracy - nn_accuracy}')\n",
    "    \n",
    "    log_accuracy = 0\n",
    "    log_accuracy_list = []\n",
    "    val_predict = np.array([])\n",
    "\n",
    "    for alpha in np.arange(0, 1.1, 0.1):\n",
    "        nn_val_predict_scaled = alpha * nn_val_predict\n",
    "        xgb_val_predict_scaled = (1 - alpha) * xgb_val_predict\n",
    "        val_predict = nn_val_predict_scaled + xgb_val_predict_scaled\n",
    "        log_accuracy = np.log(np.mean(np.abs(validation_y - val_predict)))\n",
    "        log_accuracy_list.append(log_accuracy)\n",
    "\n",
    "    return log_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1JHN out of ['1JHN', '1JHC', '2JHH', '2JHC', '2JHN', '3JHH', '3JHC', '3JHN'] at 08.14.2019_11.18.50.\n",
      "[0]\tvalidation_0-mae:16.499\n",
      "Will train until validation_0-mae hasn't improved in 32 rounds.\n",
      "[1]\tvalidation_0-mae:14.9135\n",
      "[2]\tvalidation_0-mae:13.4888\n",
      "[3]\tvalidation_0-mae:12.2127\n",
      "[4]\tvalidation_0-mae:11.0641\n",
      "[5]\tvalidation_0-mae:10.0369\n",
      "[6]\tvalidation_0-mae:9.11629\n",
      "[7]\tvalidation_0-mae:8.2859\n",
      "[8]\tvalidation_0-mae:7.53977\n",
      "[9]\tvalidation_0-mae:6.86845\n",
      "[10]\tvalidation_0-mae:6.26602\n",
      "[11]\tvalidation_0-mae:5.72436\n",
      "[12]\tvalidation_0-mae:5.23625\n",
      "[13]\tvalidation_0-mae:4.7997\n",
      "[14]\tvalidation_0-mae:4.40601\n",
      "[15]\tvalidation_0-mae:4.04733\n",
      "[16]\tvalidation_0-mae:3.72347\n",
      "[17]\tvalidation_0-mae:3.43358\n",
      "[18]\tvalidation_0-mae:3.17379\n",
      "[19]\tvalidation_0-mae:2.93891\n",
      "[20]\tvalidation_0-mae:2.72624\n",
      "[21]\tvalidation_0-mae:2.53419\n",
      "[22]\tvalidation_0-mae:2.36267\n",
      "[23]\tvalidation_0-mae:2.21112\n",
      "[24]\tvalidation_0-mae:2.07478\n",
      "[25]\tvalidation_0-mae:1.95453\n",
      "[26]\tvalidation_0-mae:1.84727\n",
      "[27]\tvalidation_0-mae:1.75194\n",
      "[28]\tvalidation_0-mae:1.66382\n",
      "[29]\tvalidation_0-mae:1.58883\n",
      "[30]\tvalidation_0-mae:1.51793\n",
      "[31]\tvalidation_0-mae:1.45951\n",
      "[32]\tvalidation_0-mae:1.4051\n",
      "[33]\tvalidation_0-mae:1.35852\n",
      "[34]\tvalidation_0-mae:1.31759\n",
      "[35]\tvalidation_0-mae:1.28576\n",
      "[36]\tvalidation_0-mae:1.25285\n",
      "[37]\tvalidation_0-mae:1.22748\n",
      "[38]\tvalidation_0-mae:1.20596\n",
      "[39]\tvalidation_0-mae:1.18494\n",
      "[40]\tvalidation_0-mae:1.16811\n",
      "[41]\tvalidation_0-mae:1.1539\n",
      "[42]\tvalidation_0-mae:1.13729\n",
      "[43]\tvalidation_0-mae:1.12474\n",
      "[44]\tvalidation_0-mae:1.10943\n",
      "[45]\tvalidation_0-mae:1.09763\n",
      "[46]\tvalidation_0-mae:1.08776\n",
      "[47]\tvalidation_0-mae:1.07752\n",
      "[48]\tvalidation_0-mae:1.06949\n",
      "[49]\tvalidation_0-mae:1.06031\n",
      "[50]\tvalidation_0-mae:1.05309\n",
      "[51]\tvalidation_0-mae:1.04651\n",
      "[52]\tvalidation_0-mae:1.03957\n",
      "[53]\tvalidation_0-mae:1.03297\n",
      "[54]\tvalidation_0-mae:1.02688\n",
      "[55]\tvalidation_0-mae:1.0216\n",
      "[56]\tvalidation_0-mae:1.01807\n",
      "[57]\tvalidation_0-mae:1.01137\n",
      "[58]\tvalidation_0-mae:1.00599\n",
      "[59]\tvalidation_0-mae:1.00177\n",
      "[60]\tvalidation_0-mae:0.998491\n",
      "[61]\tvalidation_0-mae:0.995035\n",
      "[62]\tvalidation_0-mae:0.992548\n",
      "[63]\tvalidation_0-mae:0.987976\n",
      "[64]\tvalidation_0-mae:0.983042\n",
      "[65]\tvalidation_0-mae:0.978148\n",
      "[66]\tvalidation_0-mae:0.974734\n",
      "[67]\tvalidation_0-mae:0.970493\n",
      "[68]\tvalidation_0-mae:0.967856\n",
      "[69]\tvalidation_0-mae:0.963409\n",
      "[70]\tvalidation_0-mae:0.958518\n",
      "[71]\tvalidation_0-mae:0.954785\n",
      "[72]\tvalidation_0-mae:0.950174\n",
      "[73]\tvalidation_0-mae:0.948432\n",
      "[74]\tvalidation_0-mae:0.944258\n",
      "[75]\tvalidation_0-mae:0.941729\n",
      "[76]\tvalidation_0-mae:0.937594\n",
      "[77]\tvalidation_0-mae:0.933133\n",
      "[78]\tvalidation_0-mae:0.929767\n",
      "[79]\tvalidation_0-mae:0.927472\n",
      "[80]\tvalidation_0-mae:0.922893\n",
      "[81]\tvalidation_0-mae:0.918463\n",
      "[82]\tvalidation_0-mae:0.916986\n",
      "[83]\tvalidation_0-mae:0.914501\n",
      "[84]\tvalidation_0-mae:0.911408\n",
      "[85]\tvalidation_0-mae:0.909306\n",
      "[86]\tvalidation_0-mae:0.908053\n",
      "[87]\tvalidation_0-mae:0.905198\n",
      "[88]\tvalidation_0-mae:0.900762\n",
      "[89]\tvalidation_0-mae:0.897497\n",
      "[90]\tvalidation_0-mae:0.895088\n",
      "[91]\tvalidation_0-mae:0.892777\n",
      "[92]\tvalidation_0-mae:0.890022\n",
      "[93]\tvalidation_0-mae:0.888513\n",
      "[94]\tvalidation_0-mae:0.88471\n",
      "[95]\tvalidation_0-mae:0.882756\n",
      "[96]\tvalidation_0-mae:0.880825\n",
      "[97]\tvalidation_0-mae:0.878164\n",
      "[98]\tvalidation_0-mae:0.877106\n",
      "[99]\tvalidation_0-mae:0.874822\n",
      "[100]\tvalidation_0-mae:0.873465\n",
      "[101]\tvalidation_0-mae:0.872175\n",
      "[102]\tvalidation_0-mae:0.869978\n",
      "[103]\tvalidation_0-mae:0.867589\n",
      "[104]\tvalidation_0-mae:0.866398\n",
      "[105]\tvalidation_0-mae:0.864151\n",
      "[106]\tvalidation_0-mae:0.862293\n",
      "[107]\tvalidation_0-mae:0.859698\n",
      "[108]\tvalidation_0-mae:0.856936\n",
      "[109]\tvalidation_0-mae:0.856029\n",
      "[110]\tvalidation_0-mae:0.85454\n",
      "[111]\tvalidation_0-mae:0.853393\n",
      "[112]\tvalidation_0-mae:0.851716\n",
      "[113]\tvalidation_0-mae:0.848101\n",
      "[114]\tvalidation_0-mae:0.847566\n",
      "[115]\tvalidation_0-mae:0.845244\n",
      "[116]\tvalidation_0-mae:0.841931\n",
      "[117]\tvalidation_0-mae:0.839407\n",
      "[118]\tvalidation_0-mae:0.838169\n",
      "[119]\tvalidation_0-mae:0.836091\n",
      "[120]\tvalidation_0-mae:0.833958\n",
      "[121]\tvalidation_0-mae:0.831649\n",
      "[122]\tvalidation_0-mae:0.830091\n",
      "[123]\tvalidation_0-mae:0.828976\n",
      "[124]\tvalidation_0-mae:0.82823\n",
      "[125]\tvalidation_0-mae:0.827295\n",
      "[126]\tvalidation_0-mae:0.82582\n",
      "[127]\tvalidation_0-mae:0.824145\n",
      "[128]\tvalidation_0-mae:0.821765\n",
      "[129]\tvalidation_0-mae:0.821429\n",
      "[130]\tvalidation_0-mae:0.819871\n",
      "[131]\tvalidation_0-mae:0.817625\n",
      "[132]\tvalidation_0-mae:0.816009\n",
      "[133]\tvalidation_0-mae:0.814685\n",
      "[134]\tvalidation_0-mae:0.813199\n",
      "[135]\tvalidation_0-mae:0.812121\n",
      "[136]\tvalidation_0-mae:0.81159\n",
      "[137]\tvalidation_0-mae:0.811158\n",
      "[138]\tvalidation_0-mae:0.809568\n",
      "[139]\tvalidation_0-mae:0.806828\n",
      "[140]\tvalidation_0-mae:0.805133\n",
      "[141]\tvalidation_0-mae:0.802725\n",
      "[142]\tvalidation_0-mae:0.80126\n",
      "[143]\tvalidation_0-mae:0.799866\n",
      "[144]\tvalidation_0-mae:0.798406\n",
      "[145]\tvalidation_0-mae:0.796567\n",
      "[146]\tvalidation_0-mae:0.795556\n",
      "[147]\tvalidation_0-mae:0.793917\n",
      "[148]\tvalidation_0-mae:0.79218\n",
      "[149]\tvalidation_0-mae:0.790453\n",
      "[150]\tvalidation_0-mae:0.790228\n",
      "[151]\tvalidation_0-mae:0.789076\n",
      "[152]\tvalidation_0-mae:0.788397\n",
      "[153]\tvalidation_0-mae:0.786341\n",
      "[154]\tvalidation_0-mae:0.784837\n",
      "[155]\tvalidation_0-mae:0.783707\n",
      "[156]\tvalidation_0-mae:0.781998\n",
      "[157]\tvalidation_0-mae:0.780861\n",
      "[158]\tvalidation_0-mae:0.779651\n",
      "[159]\tvalidation_0-mae:0.778162\n",
      "[160]\tvalidation_0-mae:0.776591\n",
      "[161]\tvalidation_0-mae:0.775023\n",
      "[162]\tvalidation_0-mae:0.773546\n",
      "[163]\tvalidation_0-mae:0.772068\n",
      "[164]\tvalidation_0-mae:0.770987\n",
      "[165]\tvalidation_0-mae:0.769756\n",
      "[166]\tvalidation_0-mae:0.769191\n",
      "[167]\tvalidation_0-mae:0.768752\n",
      "[168]\tvalidation_0-mae:0.767307\n",
      "[169]\tvalidation_0-mae:0.766399\n",
      "[170]\tvalidation_0-mae:0.765134\n",
      "[171]\tvalidation_0-mae:0.764079\n",
      "[172]\tvalidation_0-mae:0.763161\n",
      "[173]\tvalidation_0-mae:0.762237\n",
      "[174]\tvalidation_0-mae:0.761318\n",
      "[175]\tvalidation_0-mae:0.760464\n",
      "[176]\tvalidation_0-mae:0.758578\n",
      "[177]\tvalidation_0-mae:0.757379\n",
      "[178]\tvalidation_0-mae:0.756247\n",
      "[179]\tvalidation_0-mae:0.755567\n",
      "[180]\tvalidation_0-mae:0.75497\n",
      "[181]\tvalidation_0-mae:0.753658\n",
      "[182]\tvalidation_0-mae:0.752407\n",
      "[183]\tvalidation_0-mae:0.751205\n",
      "[184]\tvalidation_0-mae:0.750495\n",
      "[185]\tvalidation_0-mae:0.749985\n",
      "[186]\tvalidation_0-mae:0.748962\n",
      "[187]\tvalidation_0-mae:0.748229\n",
      "[188]\tvalidation_0-mae:0.747905\n",
      "[189]\tvalidation_0-mae:0.747725\n",
      "[190]\tvalidation_0-mae:0.746574\n",
      "[191]\tvalidation_0-mae:0.745362\n",
      "[192]\tvalidation_0-mae:0.744353\n",
      "[193]\tvalidation_0-mae:0.74351\n",
      "[194]\tvalidation_0-mae:0.742724\n",
      "[195]\tvalidation_0-mae:0.742335\n",
      "[196]\tvalidation_0-mae:0.741625\n",
      "[197]\tvalidation_0-mae:0.740499\n",
      "[198]\tvalidation_0-mae:0.739611\n",
      "[199]\tvalidation_0-mae:0.738873\n",
      "[200]\tvalidation_0-mae:0.738424\n",
      "[201]\tvalidation_0-mae:0.738066\n",
      "[202]\tvalidation_0-mae:0.736846\n",
      "[203]\tvalidation_0-mae:0.735865\n",
      "[204]\tvalidation_0-mae:0.735269\n",
      "[205]\tvalidation_0-mae:0.734672\n",
      "[206]\tvalidation_0-mae:0.733092\n",
      "[207]\tvalidation_0-mae:0.732513\n",
      "[208]\tvalidation_0-mae:0.73143\n",
      "[209]\tvalidation_0-mae:0.730771\n",
      "[210]\tvalidation_0-mae:0.729704\n",
      "[211]\tvalidation_0-mae:0.728314\n",
      "[212]\tvalidation_0-mae:0.72725\n",
      "[213]\tvalidation_0-mae:0.726654\n",
      "[214]\tvalidation_0-mae:0.726125\n",
      "[215]\tvalidation_0-mae:0.724585\n",
      "[216]\tvalidation_0-mae:0.723989\n",
      "[217]\tvalidation_0-mae:0.723141\n",
      "[218]\tvalidation_0-mae:0.722442\n",
      "[219]\tvalidation_0-mae:0.721772\n",
      "[220]\tvalidation_0-mae:0.720737\n",
      "[221]\tvalidation_0-mae:0.720132\n",
      "[222]\tvalidation_0-mae:0.719803\n",
      "[223]\tvalidation_0-mae:0.719099\n",
      "[224]\tvalidation_0-mae:0.718594\n",
      "[225]\tvalidation_0-mae:0.717794\n",
      "[226]\tvalidation_0-mae:0.716635\n",
      "[227]\tvalidation_0-mae:0.715865\n",
      "[228]\tvalidation_0-mae:0.714776\n",
      "[229]\tvalidation_0-mae:0.713514\n",
      "[230]\tvalidation_0-mae:0.713302\n",
      "[231]\tvalidation_0-mae:0.712317\n",
      "[232]\tvalidation_0-mae:0.711627\n",
      "[233]\tvalidation_0-mae:0.710906\n",
      "[234]\tvalidation_0-mae:0.709998\n",
      "[235]\tvalidation_0-mae:0.709463\n",
      "[236]\tvalidation_0-mae:0.708564\n",
      "[237]\tvalidation_0-mae:0.707927\n",
      "[238]\tvalidation_0-mae:0.707216\n",
      "[239]\tvalidation_0-mae:0.706877\n",
      "[240]\tvalidation_0-mae:0.705973\n",
      "[241]\tvalidation_0-mae:0.705229\n",
      "[242]\tvalidation_0-mae:0.704651\n",
      "[243]\tvalidation_0-mae:0.704278\n",
      "[244]\tvalidation_0-mae:0.704087\n",
      "[245]\tvalidation_0-mae:0.703843\n",
      "[246]\tvalidation_0-mae:0.702915\n",
      "[247]\tvalidation_0-mae:0.702244\n",
      "[248]\tvalidation_0-mae:0.701752\n",
      "[249]\tvalidation_0-mae:0.701257\n",
      "[250]\tvalidation_0-mae:0.700367\n",
      "[251]\tvalidation_0-mae:0.699732\n",
      "[252]\tvalidation_0-mae:0.699002\n",
      "[253]\tvalidation_0-mae:0.698099\n",
      "[254]\tvalidation_0-mae:0.697727\n",
      "[255]\tvalidation_0-mae:0.697123\n",
      "[256]\tvalidation_0-mae:0.696822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[257]\tvalidation_0-mae:0.696544\n"
     ]
    }
   ],
   "source": [
    "load = False\n",
    "\n",
    "df_test = pd.read_csv('../submissions/submission_best.csv')\n",
    "test_prediction = df_test['scalar_coupling_constant']\n",
    "df_test_full = pd.read_csv('../input/test.csv')\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "val_score = {\n",
    "    '1JHC': np.inf, '1JHN': np.inf, '2JHH': np.inf, '2JHC': np.inf, \n",
    "    '2JHN': np.inf, '3JHH': np.inf, '3JHC': np.inf, '3JHN': np.inf\n",
    "}\n",
    "\n",
    "coupling_types = ['1JHN', '1JHC', '2JHH', '2JHC', '2JHN', '3JHH', '3JHC', '3JHN']\n",
    "\n",
    "for coupling_type in ['1JHN']:\n",
    "    time = datetime.now(pytz.timezone('Europe/Oslo')).strftime('%m.%d.%Y_%H.%M.%S')\n",
    "    print(f'Predicting {coupling_type} out of {coupling_types} at {time}.')\n",
    "    \n",
    "    df_train, df_test = get_features(coupling_type)\n",
    "    train_X, train_y, validation_X, validation_y = split(df_train)\n",
    "    \n",
    "    ################################# NN #################################\n",
    "    '''\n",
    "    if load:         \n",
    "        model_name_rd_nn = f'models/nn/nn_model_{coupling_type}.hdf5'\n",
    "        nn_model = create_nn_model(validation_X.shape[1]) # vals\n",
    "        print(f'Loading weights from {model_name_rd_nn}.')\n",
    "        nn_model.load_weights(model_name_rd_nn)\n",
    "    else:\n",
    "        nn_model = nn_train(coupling_type, train_X, train_y, validation_X, validation_y)\n",
    "    \n",
    "    nn_val_predict = nn_model.predict(validation_X)\n",
    "    nn_accuracy = np.log(np.mean(np.abs(validation_y.values - nn_val_predict)))\n",
    "    print(f'Validation score for {coupling_type} is {nn_accuracy} with NN.\\n')\n",
    "    '''\n",
    "    ################################# XGB #################################\n",
    "\n",
    "    if load:\n",
    "        model_name_rd_xgb = f'..models/xgb/featurebook_{coupling_type}.joblib.dat'\n",
    "\n",
    "        xgb_model = xgb.XGBRegressor(base_score = 0.5, booster = 'gbtree', colsample_bylevel = 1,\n",
    "                                     colsample_bytree = 1, gamma = 0, importance_type = 'gain',\n",
    "                                     learning_rate = 0.1, max_delta_step = 0, max_depth = 9,\n",
    "                                     min_child_weight = 1, missing = None, n_estimators = 10000, n_jobs = -1,\n",
    "                                     nthread = None, objective = 'reg:squarederror', random_state = 101, reg_alpha = 2,\n",
    "                                     reg_lambda = 0.2, scale_pos_weight = 1, seed = None, silent = False, subsample = 1)\n",
    "\n",
    "        print(f'Loading weights from {model_name_rd_xgb}.')\n",
    "        #xgb_model.load_model(model_name_rd_xgb)\n",
    "        xgb_model= joblib.load(model_name_rd_xgb)\n",
    "    else:\n",
    "        xgb_model = xgb_train(coupling_type, train_X, train_y, validation_X, validation_y)   \n",
    "        \n",
    "    xgb_val_predict = xgb_model.predict(validation_X)\n",
    "    memory_optimization([train_X, train_y, validation_X])\n",
    "    validation_y = validation_y['scalar_coupling_constant'].values\n",
    "\n",
    "    diff = validation_y - xgb_val_predict\n",
    "    xgb_accuracy = np.log(np.mean(np.abs(diff)))\n",
    "\n",
    "    print(f'Validation score for {coupling_type} is {xgb_accuracy} with XGB.\\n')\n",
    "    '''\n",
    "    ################################# BLEND #################################\n",
    "        \n",
    "    nn_val_predict = np.transpose(nn_val_predict[:,0])\n",
    "    log_accuracy_list = val_blending_list(nn_val_predict, xgb_val_predict, validation_y)\n",
    "    print(f'Accuracy_list with alphas for {coupling_type}:\\n {log_accuracy_list}')\n",
    "    \n",
    "    alpha_i = np.argmin(log_accuracy_list)\n",
    "    log_accuracy = log_accuracy_list[alpha_i]\n",
    "    val_score[coupling_type] = (log_accuracy)\n",
    "    alpha = alpha_i * 0.1\n",
    "    print(f'Blending with alpha = {alpha}, final accuracy for {coupling_type} = {log_accuracy}.') \n",
    "    \n",
    "    ################################# PREDICT #################################\n",
    "    \n",
    "    print('Predicting NN:')\n",
    "    nn_test_predict = nn_model.predict(df_test)\n",
    "    nn_test_predict_scaled = alpha * nn_test_predict\n",
    "    nn_test_predict_scaled = np.transpose(nn_test_predict_scaled[:, 0])\n",
    "    '''\n",
    "    print('Predicting XGB:')\n",
    "    xgb_test_predict = xgb_model.predict(df_test)\n",
    "    xgb_test_predict_scaled = (1 - alpha) * xgb_test_predict\n",
    "    \n",
    "    #test_predict = nn_test_predict_scaled + xgb_test_predict_scaled\n",
    "    test_predict = xgb_test_predict_scaled\n",
    "    test_prediction[df_test_full['type'] == coupling_type] = test_predict\n",
    "\n",
    "    #memory_optimization([df_test, nn_model, nn_val_predict, xgb_model, xgb_val_predict, nn_test_predict, \n",
    "    #                     nn_test_predict_scaled, xgb_test_predict, xgb_test_predict_scaled, test_predict])\n",
    "    \n",
    "val_score_total = sum(val_score.values()) / len(val_score.keys())\n",
    "print(f'Total cv score is {val_score_total}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(predictions):\n",
    "    submit = pd.read_csv('submissions/submission_best.csv')  \n",
    "    submit['scalar_coupling_constant'] = predictions\n",
    "    submit.to_csv('submissions/submission_blended_full.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(test_prediction)\n",
    "\n",
    "time = datetime.now(pytz.timezone('Europe/Oslo')).strftime('%m.%d.%Y_%H.%M.%S')\n",
    "print(f'Notebook EoF reached at {time} and submission saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
