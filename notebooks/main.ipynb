{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pytz\n",
    "import operator\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import callbacks\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.losses import mean_absolute_error\n",
    "from tensorflow.python.keras.layers import Dense, Input, Activation\n",
    "from tensorflow.python.keras.layers import BatchNormalization, Add, Dropout\n",
    "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.python.keras.optimizers import Adam, Adadelta, SGD\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action = 'ignore', category = FutureWarning)\n",
    "warnings.filterwarnings(action = 'ignore', category = DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.generate_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'08.13.2019_12.24.30'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now(pytz.timezone('Europe/Oslo')).strftime('%m.%d.%Y_%H.%M.%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_optimization(dfs):\n",
    "    for df in dfs:\n",
    "        del df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df_train):\n",
    "    train_X, validation_X = train_test_split(df_train, test_size = 0.1, random_state = 0)\n",
    "\n",
    "    train_X = train_X.reset_index()\n",
    "    validation_X = validation_X.reset_index()\n",
    "\n",
    "    train_y = train_X['scalar_coupling_constant']\n",
    "    train_y = train_y.replace([np.inf, -np.inf], np.nan)\n",
    "    train_y = train_y.reset_index()\n",
    "    train_y = train_y.drop(['index'], axis = 1)\n",
    "    validation_y = validation_X['scalar_coupling_constant']\n",
    "    validation_y = validation_y.replace([np.inf, -np.inf], np.nan)\n",
    "    validation_y = validation_y.reset_index()\n",
    "    validation_y = validation_y.drop(['index'], axis = 1)\n",
    "\n",
    "    train_X = train_X.drop('scalar_coupling_constant', axis = 1)\n",
    "    validation_X = validation_X.drop('scalar_coupling_constant', axis = 1)\n",
    "    \n",
    "    train_X = train_X.drop(['index'], axis = 1)\n",
    "    validation_X = validation_X.drop(['index'], axis = 1)\n",
    "    \n",
    "    return train_X, train_y, validation_X, validation_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    # input layer\n",
    "    inp = Input(shape = (input_shape,))\n",
    "\n",
    "    # first hidden layer\n",
    "    x = Dense(256, kernel_initializer = 'he_normal')(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # second hidden layer\n",
    "    x = Dense(512, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # third hidden layer\n",
    "    x = Dense(1024, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # fourth hidden layer\n",
    "    x = Dense(512, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # fifth hidden layer\n",
    "    x = Dense(256, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # sixth hidden layer\n",
    "    x = Dense(128, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # seventh hidden layer\n",
    "    x = Dense(128, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # eight hidden layer\n",
    "    x = Dense(64, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # output layer scalar_coupling_constant\n",
    "    out = Dense(1, activation = 'linear')(x)   \n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(coupling_type, train_X, train_y, validation_X, validation_y):\n",
    "    epoch_n = 2000\n",
    "    verbose = 1\n",
    "    batch_size = 2048\n",
    "    model_name = f'../models/nn/coupling_model_{coupling_type}_NN.hdf5'\n",
    "    \n",
    "    nn_model = create_nn_model(train_X.shape[1])\n",
    "\n",
    "    nn_model.compile(loss = 'mae', optimizer = Adam())\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor = 'loss', min_delta = 0.00001, patience = 64,\n",
    "                                 verbose = verbose, mode = 'auto', restore_best_weights = True)\n",
    "    \n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 32,\n",
    "                                      min_lr = 1e-7, mode = 'auto', verbose = verbose)\n",
    "    \n",
    "    sv_mod = callbacks.ModelCheckpoint(model_name, monitor = 'val_loss', save_best_only = True,\n",
    "                                       save_weights_only = True, restore_best_weights = True)\n",
    "    \n",
    "    history = nn_model.fit(x = train_X.values, y = train_y['scalar_coupling_constant'].values, \n",
    "                           validation_data = (validation_X.values, validation_y['scalar_coupling_constant'].values),\n",
    "                           callbacks = [es, rlr, sv_mod], epochs = epoch_n, batch_size = batch_size, verbose = verbose)\n",
    "    \n",
    "    nn_model.save_weights(model_name)\n",
    "\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_train(coupling_type, train_X, train_y, validation_X, validation_y):\n",
    "    model_name_wrt = f'../models/xgb/coupling_model_{coupling_type}_XGB.hdf5'\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(base_score = 0.5, booster = 'gbtree', colsample_bylevel = 1,\n",
    "                                 colsample_bytree = 1, gamma = 0, importance_type = 'gain',\n",
    "                                 learning_rate = 0.1, max_delta_step = 0, max_depth = 9,\n",
    "                                 min_child_weight = 1, missing = None, n_estimators = 10000, n_jobs = -1,\n",
    "                                 nthread = None, objective = 'reg:squarederror', random_state = 101, reg_alpha = 2,\n",
    "                                 reg_lambda = 0.2, scale_pos_weight = 1, seed = None, silent = False, subsample = 1)\n",
    "\n",
    "    xgb_model.fit(train_X, train_y, eval_set = [(validation_X, validation_y)], eval_metric = 'mae', \n",
    "                  early_stopping_rounds = 32, verbose = True)   \n",
    "    \n",
    "    xgb_model.save_model(model_name_wrt)\n",
    "    #joblib.dump(xgb_model, model_name_wrt)\n",
    "    \n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance(xgb_model, train_X):\n",
    "    input_features = train_X.columns.values\n",
    "    feat_imp = xgb_model.feature_importances_\n",
    "    np.split(feat_imp, len(input_features))\n",
    "    \n",
    "    feat_imp_dict = {}\n",
    "    for i in range(0, len(input_features)):\n",
    "        feat_imp_dict[feat_imp[i]] = input_features[i]\n",
    "\n",
    "    sorted_feats = sorted(feat_imp_dict.items(), key = operator.itemgetter(0))\n",
    "    for i in range(len(sorted_feats) - 1, 0, -1):\n",
    "        print(sorted_feats[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Blending and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_blending_list(nn_val_predict, xgb_val_predict, validation_y):\n",
    "    nn_val_pred_error  = (validation_y - nn_val_predict)\n",
    "    xgb_val_pred_error = (validation_y - xgb_val_predict)\n",
    "    \n",
    "    val_error_corr = np.corrcoef(nn_val_pred_error, xgb_val_pred_error)\n",
    "    print(f'Error correlation: {val_error_corr[0][1]}. Error difference {xgb_accuracy - nn_accuracy}')\n",
    "    \n",
    "    log_accuracy = 0\n",
    "    log_accuracy_list = []\n",
    "    val_predict = np.array([])\n",
    "\n",
    "    for alpha in np.arange(0, 1.1, 0.1):\n",
    "        nn_val_predict_scaled = alpha * nn_val_predict\n",
    "        xgb_val_predict_scaled = (1 - alpha) * xgb_val_predict\n",
    "        val_predict = nn_val_predict_scaled + xgb_val_predict_scaled\n",
    "        log_accuracy = np.log(np.mean(np.abs(validation_y - val_predict)))\n",
    "        log_accuracy_list.append(log_accuracy)\n",
    "\n",
    "    return log_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1JHN out of ['1JHN', '1JHC', '2JHH', '2JHC', '2JHN', '3JHH', '3JHC', '3JHN'] at 08.13.2019_12.24.33\n",
      "Train on 39026 samples, validate on 4337 samples\n",
      "Epoch 1/2000\n",
      "39026/39026 [==============================] - 15s 384us/sample - loss: 47.0821 - val_loss: 29.6951\n",
      "Epoch 2/2000\n",
      "39026/39026 [==============================] - 8s 213us/sample - loss: 46.1115 - val_loss: 34.6080\n",
      "Epoch 3/2000\n",
      "39026/39026 [==============================] - 9s 232us/sample - loss: 45.1914 - val_loss: 39.4792\n",
      "Epoch 4/2000\n",
      "39026/39026 [==============================] - 9s 221us/sample - loss: 44.2320 - val_loss: 50.6680\n",
      "Epoch 5/2000\n",
      "39026/39026 [==============================] - 10s 260us/sample - loss: 43.2961 - val_loss: 39.3412\n",
      "Epoch 6/2000\n",
      "39026/39026 [==============================] - 10s 245us/sample - loss: 42.3933 - val_loss: 30.2236\n",
      "Epoch 7/2000\n",
      "39026/39026 [==============================] - 9s 240us/sample - loss: 41.4403 - val_loss: 32.7161\n",
      "Epoch 8/2000\n",
      "39026/39026 [==============================] - 9s 232us/sample - loss: 40.4357 - val_loss: 35.2296\n",
      "Epoch 9/2000\n",
      "39026/39026 [==============================] - 9s 239us/sample - loss: 39.3716 - val_loss: 35.1632\n",
      "Epoch 10/2000\n",
      "39026/39026 [==============================] - 9s 223us/sample - loss: 38.2109 - val_loss: 35.9256\n",
      "Epoch 11/2000\n",
      "39026/39026 [==============================] - 9s 230us/sample - loss: 36.9989 - val_loss: 34.6105\n",
      "Epoch 12/2000\n",
      "39026/39026 [==============================] - 9s 223us/sample - loss: 35.6421 - val_loss: 33.9399\n",
      "Epoch 13/2000\n",
      "39026/39026 [==============================] - 9s 243us/sample - loss: 34.2301 - val_loss: 32.6378\n",
      "Epoch 14/2000\n",
      "39026/39026 [==============================] - 9s 234us/sample - loss: 32.7212 - val_loss: 31.7291\n",
      "Epoch 15/2000\n",
      "39026/39026 [==============================] - 9s 237us/sample - loss: 31.0694 - val_loss: 29.7289\n",
      "Epoch 16/2000\n",
      "39026/39026 [==============================] - 9s 242us/sample - loss: 29.3328 - val_loss: 27.6932\n",
      "Epoch 17/2000\n",
      "39026/39026 [==============================] - 9s 219us/sample - loss: 27.5059 - val_loss: 25.9555\n",
      "Epoch 18/2000\n",
      "39026/39026 [==============================] - 8s 216us/sample - loss: 25.5307 - val_loss: 22.5897\n",
      "Epoch 19/2000\n",
      "39026/39026 [==============================] - 9s 225us/sample - loss: 23.4220 - val_loss: 27.7230\n",
      "Epoch 20/2000\n",
      "39026/39026 [==============================] - 9s 220us/sample - loss: 21.1314 - val_loss: 33.4036\n",
      "Epoch 21/2000\n",
      "39026/39026 [==============================] - 9s 227us/sample - loss: 18.7699 - val_loss: 28.6623\n",
      "Epoch 22/2000\n",
      "39026/39026 [==============================] - 9s 235us/sample - loss: 16.5205 - val_loss: 20.7375\n",
      "Epoch 23/2000\n",
      "39026/39026 [==============================] - 9s 235us/sample - loss: 14.6267 - val_loss: 14.0544\n",
      "Epoch 24/2000\n",
      "39026/39026 [==============================] - 9s 237us/sample - loss: 13.1356 - val_loss: 12.9204\n",
      "Epoch 25/2000\n",
      "39026/39026 [==============================] - 9s 238us/sample - loss: 12.0309 - val_loss: 11.0030\n",
      "Epoch 26/2000\n",
      "39026/39026 [==============================] - 9s 237us/sample - loss: 11.1750 - val_loss: 10.6087\n",
      "Epoch 27/2000\n",
      "39026/39026 [==============================] - 9s 239us/sample - loss: 10.5671 - val_loss: 9.5995\n",
      "Epoch 28/2000\n",
      "39026/39026 [==============================] - 9s 236us/sample - loss: 10.1915 - val_loss: 9.3281\n",
      "Epoch 29/2000\n",
      "39026/39026 [==============================] - 9s 239us/sample - loss: 9.9122 - val_loss: 9.2399\n",
      "Epoch 30/2000\n",
      "39026/39026 [==============================] - 9s 242us/sample - loss: 9.7385 - val_loss: 9.4541\n",
      "Epoch 31/2000\n",
      "39026/39026 [==============================] - 9s 243us/sample - loss: 9.6575 - val_loss: 9.0586\n",
      "Epoch 32/2000\n",
      "39026/39026 [==============================] - 9s 236us/sample - loss: 9.5571 - val_loss: 9.3432\n",
      "Epoch 33/2000\n",
      "39026/39026 [==============================] - 9s 238us/sample - loss: 9.5507 - val_loss: 9.2296\n",
      "Epoch 34/2000\n",
      "39026/39026 [==============================] - 9s 240us/sample - loss: 9.4685 - val_loss: 9.3772\n",
      "Epoch 35/2000\n",
      "39026/39026 [==============================] - 9s 242us/sample - loss: 9.4917 - val_loss: 9.4151\n",
      "Epoch 36/2000\n",
      "39026/39026 [==============================] - 9s 239us/sample - loss: 9.3920 - val_loss: 9.4941\n",
      "Epoch 37/2000\n",
      "39026/39026 [==============================] - 9s 238us/sample - loss: 9.3968 - val_loss: 9.5106\n",
      "Epoch 38/2000\n",
      "39026/39026 [==============================] - 9s 236us/sample - loss: 9.3731 - val_loss: 9.4371\n",
      "Epoch 39/2000\n",
      "39026/39026 [==============================] - 9s 230us/sample - loss: 9.3608 - val_loss: 9.5119\n",
      "Epoch 40/2000\n",
      "39026/39026 [==============================] - 9s 235us/sample - loss: 9.3560 - val_loss: 9.3429\n",
      "Epoch 41/2000\n",
      "39026/39026 [==============================] - 9s 240us/sample - loss: 9.3060 - val_loss: 9.6322\n",
      "Epoch 42/2000\n",
      "39026/39026 [==============================] - 9s 228us/sample - loss: 9.2714 - val_loss: 9.2761\n",
      "Epoch 43/2000\n",
      "39026/39026 [==============================] - 9s 230us/sample - loss: 9.3020 - val_loss: 9.3957\n",
      "Epoch 44/2000\n",
      "39026/39026 [==============================] - 9s 233us/sample - loss: 9.2864 - val_loss: 9.3873\n",
      "Epoch 45/2000\n",
      "39026/39026 [==============================] - 9s 237us/sample - loss: 9.2706 - val_loss: 9.3598\n",
      "Epoch 46/2000\n",
      "39026/39026 [==============================] - 9s 238us/sample - loss: 9.2676 - val_loss: 9.2996\n",
      "Epoch 47/2000\n",
      "39026/39026 [==============================] - 9s 231us/sample - loss: 9.2778 - val_loss: 9.2380\n",
      "Epoch 48/2000\n",
      "39026/39026 [==============================] - 9s 236us/sample - loss: 9.3112 - val_loss: 9.2852\n",
      "Epoch 49/2000\n",
      "39026/39026 [==============================] - 9s 231us/sample - loss: 9.2543 - val_loss: 9.3021\n",
      "Epoch 50/2000\n",
      "39026/39026 [==============================] - 9s 233us/sample - loss: 9.3462 - val_loss: 9.3955\n",
      "Epoch 51/2000\n",
      "39026/39026 [==============================] - 9s 228us/sample - loss: 9.2644 - val_loss: 9.1639\n",
      "Epoch 52/2000\n",
      "39026/39026 [==============================] - 9s 243us/sample - loss: 9.2140 - val_loss: 9.3223\n",
      "Epoch 53/2000\n",
      "39026/39026 [==============================] - 10s 255us/sample - loss: 9.2959 - val_loss: 9.0571\n",
      "Epoch 54/2000\n",
      "39026/39026 [==============================] - 10s 258us/sample - loss: 9.2788 - val_loss: 9.3688\n",
      "Epoch 55/2000\n",
      "39026/39026 [==============================] - 11s 274us/sample - loss: 9.2520 - val_loss: 9.2458\n",
      "Epoch 56/2000\n",
      "39026/39026 [==============================] - 10s 266us/sample - loss: 9.2497 - val_loss: 9.3937\n",
      "Epoch 57/2000\n",
      "39026/39026 [==============================] - 11s 279us/sample - loss: 9.1679 - val_loss: 9.2090\n",
      "Epoch 58/2000\n",
      "39026/39026 [==============================] - 10s 263us/sample - loss: 9.2120 - val_loss: 9.2601\n",
      "Epoch 59/2000\n",
      "39026/39026 [==============================] - 10s 245us/sample - loss: 9.2392 - val_loss: 9.2857\n",
      "Epoch 60/2000\n",
      "39026/39026 [==============================] - 9s 233us/sample - loss: 9.2299 - val_loss: 9.1461\n",
      "Epoch 61/2000\n",
      "39026/39026 [==============================] - 9s 226us/sample - loss: 9.2227 - val_loss: 9.0342\n",
      "Epoch 62/2000\n",
      "39026/39026 [==============================] - 9s 242us/sample - loss: 9.2646 - val_loss: 9.1019\n",
      "Epoch 63/2000\n",
      "39026/39026 [==============================] - 10s 246us/sample - loss: 9.2074 - val_loss: 9.1500\n",
      "Epoch 64/2000\n",
      "39026/39026 [==============================] - 10s 259us/sample - loss: 9.2656 - val_loss: 9.1323\n",
      "Epoch 65/2000\n",
      "39026/39026 [==============================] - 9s 238us/sample - loss: 9.2394 - val_loss: 9.0532\n",
      "Epoch 66/2000\n",
      "39026/39026 [==============================] - 10s 244us/sample - loss: 9.2162 - val_loss: 8.9523\n",
      "Epoch 67/2000\n",
      "39026/39026 [==============================] - 10s 247us/sample - loss: 9.2007 - val_loss: 8.5045\n",
      "Epoch 68/2000\n",
      "39026/39026 [==============================] - 10s 251us/sample - loss: 9.2071 - val_loss: 8.3899\n",
      "Epoch 69/2000\n",
      "39026/39026 [==============================] - 9s 238us/sample - loss: 9.2288 - val_loss: 8.4160\n",
      "Epoch 70/2000\n",
      "39026/39026 [==============================] - 10s 244us/sample - loss: 9.1935 - val_loss: 8.5483\n",
      "Epoch 71/2000\n",
      "39026/39026 [==============================] - 9s 241us/sample - loss: 9.2106 - val_loss: 9.1033\n",
      "Epoch 72/2000\n",
      "39026/39026 [==============================] - 10s 249us/sample - loss: 9.2288 - val_loss: 8.7241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/2000\n",
      "39026/39026 [==============================] - 12s 300us/sample - loss: 9.1526 - val_loss: 8.6008\n",
      "Epoch 74/2000\n",
      "39026/39026 [==============================] - 9s 239us/sample - loss: 9.1357 - val_loss: 9.3986\n",
      "Epoch 75/2000\n",
      "39026/39026 [==============================] - 10s 252us/sample - loss: 9.2055 - val_loss: 8.9691\n",
      "Epoch 76/2000\n",
      "39026/39026 [==============================] - 9s 228us/sample - loss: 9.1553 - val_loss: 8.6877\n",
      "Epoch 77/2000\n",
      "39026/39026 [==============================] - 9s 230us/sample - loss: 9.1376 - val_loss: 8.9515\n",
      "Epoch 78/2000\n",
      "39026/39026 [==============================] - 9s 231us/sample - loss: 9.1204 - val_loss: 8.9834\n",
      "Epoch 79/2000\n",
      "39026/39026 [==============================] - 9s 229us/sample - loss: 9.1440 - val_loss: 8.5056\n",
      "Epoch 80/2000\n",
      "39026/39026 [==============================] - 9s 227us/sample - loss: 9.1692 - val_loss: 9.2283\n",
      "Epoch 81/2000\n",
      "39026/39026 [==============================] - 9s 231us/sample - loss: 9.1443 - val_loss: 8.5026\n",
      "Epoch 82/2000\n",
      "39026/39026 [==============================] - 10s 246us/sample - loss: 9.1463 - val_loss: 9.5470\n",
      "Epoch 83/2000\n",
      "39026/39026 [==============================] - 9s 235us/sample - loss: 9.1465 - val_loss: 8.8206\n",
      "Epoch 84/2000\n",
      "39026/39026 [==============================] - 9s 229us/sample - loss: 9.2145 - val_loss: 8.8040\n",
      "Epoch 85/2000\n",
      "39026/39026 [==============================] - 9s 227us/sample - loss: 9.1705 - val_loss: 8.9789\n",
      "Epoch 86/2000\n",
      "39026/39026 [==============================] - 9s 233us/sample - loss: 9.1655 - val_loss: 8.8185\n",
      "Epoch 87/2000\n",
      "39026/39026 [==============================] - 9s 231us/sample - loss: 9.1839 - val_loss: 8.9061\n",
      "Epoch 88/2000\n",
      "39026/39026 [==============================] - 9s 239us/sample - loss: 9.1292 - val_loss: 9.3341\n",
      "Epoch 89/2000\n",
      "39026/39026 [==============================] - 9s 231us/sample - loss: 9.1462 - val_loss: 9.2318\n",
      "Epoch 90/2000\n",
      "39026/39026 [==============================] - 9s 236us/sample - loss: 9.1440 - val_loss: 9.1219\n",
      "Epoch 91/2000\n",
      "39026/39026 [==============================] - 9s 232us/sample - loss: 9.1406 - val_loss: 8.9959\n",
      "Epoch 92/2000\n",
      "39026/39026 [==============================] - 9s 229us/sample - loss: 9.1561 - val_loss: 8.8723\n",
      "Epoch 93/2000\n",
      "39026/39026 [==============================] - 9s 226us/sample - loss: 9.1670 - val_loss: 8.6984\n",
      "Epoch 94/2000\n",
      "39026/39026 [==============================] - 9s 231us/sample - loss: 9.1532 - val_loss: 8.3803\n",
      "Epoch 95/2000\n",
      "39026/39026 [==============================] - 9s 235us/sample - loss: 9.1194 - val_loss: 8.5214\n",
      "Epoch 96/2000\n",
      "39026/39026 [==============================] - 9s 229us/sample - loss: 9.1605 - val_loss: 8.4759\n",
      "Epoch 97/2000\n",
      "39026/39026 [==============================] - 9s 230us/sample - loss: 9.1313 - val_loss: 8.3330\n",
      "Epoch 98/2000\n",
      "39026/39026 [==============================] - 9s 233us/sample - loss: 9.1270 - val_loss: 8.4582\n",
      "Epoch 99/2000\n",
      "39026/39026 [==============================] - 9s 230us/sample - loss: 9.1497 - val_loss: 8.5965\n",
      "Epoch 100/2000\n",
      "39026/39026 [==============================] - 9s 228us/sample - loss: 9.1063 - val_loss: 9.0409\n",
      "Epoch 101/2000\n",
      "39026/39026 [==============================] - 9s 226us/sample - loss: 9.0959 - val_loss: 9.4720\n",
      "Epoch 102/2000\n",
      "39026/39026 [==============================] - 9s 231us/sample - loss: 9.1397 - val_loss: 9.2349\n",
      "Epoch 103/2000\n",
      "39026/39026 [==============================] - 9s 238us/sample - loss: 9.1349 - val_loss: 8.2957\n",
      "Epoch 104/2000\n",
      "39026/39026 [==============================] - 9s 237us/sample - loss: 9.0826 - val_loss: 8.3262\n",
      "Epoch 105/2000\n",
      "39026/39026 [==============================] - 9s 223us/sample - loss: 9.1255 - val_loss: 8.2557\n",
      "Epoch 106/2000\n",
      "39026/39026 [==============================] - 9s 220us/sample - loss: 9.0673 - val_loss: 8.7343\n",
      "Epoch 107/2000\n",
      "39026/39026 [==============================] - 9s 222us/sample - loss: 9.1003 - val_loss: 8.4532\n",
      "Epoch 108/2000\n",
      "39026/39026 [==============================] - 9s 222us/sample - loss: 9.0854 - val_loss: 8.4554\n",
      "Epoch 109/2000\n",
      "39026/39026 [==============================] - 9s 228us/sample - loss: 9.0737 - val_loss: 8.6139\n",
      "Epoch 110/2000\n",
      "39026/39026 [==============================] - 9s 227us/sample - loss: 9.0815 - val_loss: 8.3594\n",
      "Epoch 111/2000\n",
      "39026/39026 [==============================] - 9s 222us/sample - loss: 9.0572 - val_loss: 8.8953\n",
      "Epoch 112/2000\n",
      "39026/39026 [==============================] - 9s 224us/sample - loss: 9.0661 - val_loss: 9.8079\n",
      "Epoch 113/2000\n",
      "39026/39026 [==============================] - 9s 222us/sample - loss: 9.0892 - val_loss: 9.0429\n",
      "Epoch 114/2000\n",
      "39026/39026 [==============================] - 9s 221us/sample - loss: 9.1045 - val_loss: 8.3358\n",
      "Epoch 115/2000\n",
      "39026/39026 [==============================] - 9s 222us/sample - loss: 9.0946 - val_loss: 8.3131\n",
      "Epoch 116/2000\n",
      "39026/39026 [==============================] - 9s 229us/sample - loss: 9.1257 - val_loss: 8.2553\n",
      "Epoch 117/2000\n",
      "39026/39026 [==============================] - 11s 287us/sample - loss: 9.0513 - val_loss: 9.3466\n",
      "Epoch 118/2000\n",
      "39026/39026 [==============================] - 10s 259us/sample - loss: 9.0726 - val_loss: 9.6238\n",
      "Epoch 119/2000\n",
      "39026/39026 [==============================] - 10s 248us/sample - loss: 9.0634 - val_loss: 9.9058\n",
      "Epoch 120/2000\n",
      "39026/39026 [==============================] - 10s 251us/sample - loss: 9.0496 - val_loss: 8.2640\n",
      "Epoch 121/2000\n",
      "39026/39026 [==============================] - 10s 247us/sample - loss: 9.0477 - val_loss: 8.3921\n",
      "Epoch 122/2000\n",
      "39026/39026 [==============================] - 10s 244us/sample - loss: 9.0770 - val_loss: 9.2618\n",
      "Epoch 123/2000\n",
      "39026/39026 [==============================] - 10s 247us/sample - loss: 9.0195 - val_loss: 8.8691\n",
      "Epoch 124/2000\n",
      "39026/39026 [==============================] - 9s 233us/sample - loss: 9.0611 - val_loss: 8.5852\n",
      "Epoch 125/2000\n",
      "39026/39026 [==============================] - 9s 225us/sample - loss: 9.0945 - val_loss: 8.8112\n",
      "Epoch 126/2000\n",
      "39026/39026 [==============================] - 9s 228us/sample - loss: 9.0578 - val_loss: 9.2391\n",
      "Epoch 127/2000\n",
      "39026/39026 [==============================] - 10s 253us/sample - loss: 9.0256 - val_loss: 9.5298\n",
      "Epoch 128/2000\n",
      "39026/39026 [==============================] - 10s 255us/sample - loss: 9.0979 - val_loss: 8.4394\n",
      "Epoch 129/2000\n",
      "39026/39026 [==============================] - 10s 250us/sample - loss: 9.0791 - val_loss: 8.4669\n",
      "Epoch 130/2000\n",
      "39026/39026 [==============================] - 9s 227us/sample - loss: 9.0181 - val_loss: 9.5578\n",
      "Epoch 131/2000\n",
      "39026/39026 [==============================] - 9s 234us/sample - loss: 9.0662 - val_loss: 9.5899\n",
      "Epoch 132/2000\n",
      "39026/39026 [==============================] - 9s 240us/sample - loss: 9.0152 - val_loss: 9.2064\n",
      "Epoch 133/2000\n",
      "39026/39026 [==============================] - 9s 233us/sample - loss: 8.9907 - val_loss: 8.7063\n",
      "Epoch 134/2000\n",
      "39026/39026 [==============================] - 9s 236us/sample - loss: 9.0449 - val_loss: 9.0053\n",
      "Epoch 135/2000\n",
      "39026/39026 [==============================] - 9s 234us/sample - loss: 9.0475 - val_loss: 9.0510\n",
      "Epoch 136/2000\n",
      "39026/39026 [==============================] - 9s 236us/sample - loss: 9.0532 - val_loss: 8.4789\n",
      "Epoch 137/2000\n",
      "39026/39026 [==============================] - 9s 227us/sample - loss: 9.0513 - val_loss: 8.3499\n",
      "Epoch 138/2000\n",
      "39026/39026 [==============================] - 9s 240us/sample - loss: 9.0577 - val_loss: 8.5301\n",
      "Epoch 139/2000\n",
      "39026/39026 [==============================] - 9s 238us/sample - loss: 9.0187 - val_loss: 8.5456\n",
      "Epoch 140/2000\n",
      "39026/39026 [==============================] - 10s 252us/sample - loss: 9.0127 - val_loss: 8.2894\n",
      "Epoch 141/2000\n",
      "39026/39026 [==============================] - 10s 266us/sample - loss: 9.0198 - val_loss: 8.5814\n",
      "Epoch 142/2000\n",
      "39026/39026 [==============================] - 10s 267us/sample - loss: 9.0445 - val_loss: 8.1940\n",
      "Epoch 143/2000\n",
      "39026/39026 [==============================] - 11s 280us/sample - loss: 9.0015 - val_loss: 8.2727\n",
      "Epoch 144/2000\n",
      "39026/39026 [==============================] - 10s 256us/sample - loss: 9.0601 - val_loss: 9.0708\n",
      "Epoch 145/2000\n",
      "39026/39026 [==============================] - 11s 271us/sample - loss: 9.0397 - val_loss: 8.5328\n",
      "Epoch 146/2000\n",
      "39026/39026 [==============================] - 10s 262us/sample - loss: 9.0243 - val_loss: 8.3514\n",
      "Epoch 147/2000\n",
      "39026/39026 [==============================] - 10s 249us/sample - loss: 9.0396 - val_loss: 8.4981\n",
      "Epoch 148/2000\n",
      "39026/39026 [==============================] - 9s 235us/sample - loss: 8.9769 - val_loss: 9.0908\n",
      "Epoch 149/2000\n",
      "39026/39026 [==============================] - 10s 246us/sample - loss: 9.0251 - val_loss: 9.2542\n",
      "Epoch 150/2000\n",
      "39026/39026 [==============================] - 9s 241us/sample - loss: 8.9391 - val_loss: 9.1916\n",
      "Epoch 151/2000\n",
      "39026/39026 [==============================] - 9s 235us/sample - loss: 9.0097 - val_loss: 9.2690\n",
      "Epoch 152/2000\n",
      "39026/39026 [==============================] - 9s 227us/sample - loss: 8.9540 - val_loss: 8.6813\n",
      "Epoch 153/2000\n",
      "39026/39026 [==============================] - 9s 225us/sample - loss: 8.9610 - val_loss: 9.1838\n",
      "Epoch 154/2000\n",
      "39026/39026 [==============================] - 11s 272us/sample - loss: 8.9403 - val_loss: 8.8186\n",
      "Epoch 165/2000\n",
      "39026/39026 [==============================] - 10s 245us/sample - loss: 8.8962 - val_loss: 9.0698\n",
      "Epoch 166/2000\n",
      "39026/39026 [==============================] - 9s 239us/sample - loss: 8.9372 - val_loss: 8.7762\n",
      "Epoch 167/2000\n",
      "39026/39026 [==============================] - 9s 220us/sample - loss: 8.9601 - val_loss: 8.5972\n",
      "Epoch 168/2000\n",
      "39026/39026 [==============================] - 9s 219us/sample - loss: 8.9935 - val_loss: 8.7937\n",
      "Epoch 169/2000\n",
      "39026/39026 [==============================] - 9s 224us/sample - loss: 8.9377 - val_loss: 8.4352\n",
      "Epoch 170/2000\n",
      "39026/39026 [==============================] - 8s 215us/sample - loss: 8.8583 - val_loss: 8.9262\n",
      "Epoch 271/2000\n",
      "39026/39026 [==============================] - 8s 217us/sample - loss: 8.8277 - val_loss: 8.9844\n",
      "Epoch 272/2000\n",
      "39026/39026 [==============================] - 8s 203us/sample - loss: 8.8283 - val_loss: 8.4333\n",
      "Epoch 273/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.8810 - val_loss: 8.3752\n",
      "Epoch 274/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.7925 - val_loss: 8.6520\n",
      "Epoch 275/2000\n",
      "39026/39026 [==============================] - 8s 211us/sample - loss: 8.7901 - val_loss: 8.8252\n",
      "Epoch 276/2000\n",
      "39026/39026 [==============================] - 8s 209us/sample - loss: 8.8237 - val_loss: 8.5028\n",
      "Epoch 277/2000\n",
      "39026/39026 [==============================] - 8s 214us/sample - loss: 8.7991 - val_loss: 8.7619\n",
      "Epoch 278/2000\n",
      "38912/39026 [============================>.] - ETA: 0s - loss: 8.8588\n",
      "Epoch 00278: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "39026/39026 [==============================] - 8s 208us/sample - loss: 8.8602 - val_loss: 8.7665\n",
      "Epoch 279/2000\n",
      "39026/39026 [==============================] - 8s 217us/sample - loss: 8.8285 - val_loss: 8.6552\n",
      "Epoch 280/2000\n",
      "39026/39026 [==============================] - 8s 206us/sample - loss: 8.8489 - val_loss: 8.5472\n",
      "Epoch 281/2000\n",
      "39026/39026 [==============================] - 8s 199us/sample - loss: 8.8446 - val_loss: 8.4214\n",
      "Epoch 282/2000\n",
      "39026/39026 [==============================] - 8s 217us/sample - loss: 8.8277 - val_loss: 8.3409\n",
      "Epoch 283/2000\n",
      "39026/39026 [==============================] - 9s 224us/sample - loss: 8.8024 - val_loss: 8.1936\n",
      "Epoch 284/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.8117 - val_loss: 8.0686\n",
      "Epoch 285/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.8328 - val_loss: 8.0011\n",
      "Epoch 286/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.8173 - val_loss: 7.9611\n",
      "Epoch 287/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.8031 - val_loss: 7.9294\n",
      "Epoch 288/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.7815 - val_loss: 7.9316\n",
      "Epoch 289/2000\n",
      "39026/39026 [==============================] - 8s 211us/sample - loss: 8.8056 - val_loss: 7.8971\n",
      "Epoch 290/2000\n",
      "39026/39026 [==============================] - 8s 203us/sample - loss: 8.8159 - val_loss: 7.9183\n",
      "Epoch 291/2000\n",
      "39026/39026 [==============================] - 8s 199us/sample - loss: 8.8169 - val_loss: 7.9310\n",
      "Epoch 292/2000\n",
      "39026/39026 [==============================] - 8s 197us/sample - loss: 8.8162 - val_loss: 7.9979\n",
      "Epoch 293/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.7989 - val_loss: 7.9495\n",
      "Epoch 294/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.8114 - val_loss: 7.8997\n",
      "Epoch 295/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.7971 - val_loss: 7.9009\n",
      "Epoch 296/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.8032 - val_loss: 7.8783\n",
      "Epoch 297/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.7646 - val_loss: 7.8687\n",
      "Epoch 298/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.8178 - val_loss: 7.8798\n",
      "Epoch 299/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.7927 - val_loss: 7.8819\n",
      "Epoch 300/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.8017 - val_loss: 7.8675\n",
      "Epoch 301/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.8122 - val_loss: 7.8537\n",
      "Epoch 302/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.7618 - val_loss: 7.8559\n",
      "Epoch 303/2000\n",
      "39026/39026 [==============================] - 8s 199us/sample - loss: 8.8410 - val_loss: 7.8431\n",
      "Epoch 304/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.7932 - val_loss: 7.8415\n",
      "Epoch 305/2000\n",
      "39026/39026 [==============================] - 8s 199us/sample - loss: 8.7893 - val_loss: 7.8637\n",
      "Epoch 306/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.8133 - val_loss: 7.8607\n",
      "Epoch 307/2000\n",
      "39026/39026 [==============================] - 8s 211us/sample - loss: 8.7818 - val_loss: 7.8373\n",
      "Epoch 308/2000\n",
      "39026/39026 [==============================] - 8s 210us/sample - loss: 8.7657 - val_loss: 7.8419\n",
      "Epoch 309/2000\n",
      "39026/39026 [==============================] - 8s 213us/sample - loss: 8.7566 - val_loss: 7.8489\n",
      "Epoch 310/2000\n",
      "39026/39026 [==============================] - 8s 214us/sample - loss: 8.7667 - val_loss: 7.8388\n",
      "Epoch 311/2000\n",
      "39026/39026 [==============================] - 8s 212us/sample - loss: 8.7830 - val_loss: 7.8594\n",
      "Epoch 312/2000\n",
      "39026/39026 [==============================] - 9s 221us/sample - loss: 8.7738 - val_loss: 7.8598\n",
      "Epoch 313/2000\n",
      "39026/39026 [==============================] - 9s 222us/sample - loss: 8.8023 - val_loss: 7.8233\n",
      "Epoch 314/2000\n",
      "39026/39026 [==============================] - 8s 213us/sample - loss: 8.7986 - val_loss: 7.8409\n",
      "Epoch 315/2000\n",
      "39026/39026 [==============================] - 8s 213us/sample - loss: 8.7637 - val_loss: 7.8176\n",
      "Epoch 316/2000\n",
      "39026/39026 [==============================] - 8s 210us/sample - loss: 8.7978 - val_loss: 7.8067\n",
      "Epoch 317/2000\n",
      "39026/39026 [==============================] - 8s 206us/sample - loss: 8.7743 - val_loss: 7.8184\n",
      "Epoch 318/2000\n",
      "39026/39026 [==============================] - 8s 207us/sample - loss: 8.7450 - val_loss: 7.8208\n",
      "Epoch 319/2000\n",
      "39026/39026 [==============================] - 8s 209us/sample - loss: 8.7388 - val_loss: 7.8369\n",
      "Epoch 320/2000\n",
      "39026/39026 [==============================] - 8s 210us/sample - loss: 8.7713 - val_loss: 7.8776\n",
      "Epoch 321/2000\n",
      "39026/39026 [==============================] - 9s 221us/sample - loss: 8.7376 - val_loss: 7.8781\n",
      "Epoch 322/2000\n",
      "39026/39026 [==============================] - 8s 217us/sample - loss: 8.7967 - val_loss: 7.8337\n",
      "Epoch 323/2000\n",
      "39026/39026 [==============================] - 9s 220us/sample - loss: 8.7271 - val_loss: 7.8757\n",
      "Epoch 324/2000\n",
      "39026/39026 [==============================] - 9s 219us/sample - loss: 8.7485 - val_loss: 7.8316\n",
      "Epoch 325/2000\n",
      "39026/39026 [==============================] - 9s 218us/sample - loss: 8.7225 - val_loss: 7.8192\n",
      "Epoch 326/2000\n",
      "39026/39026 [==============================] - 8s 211us/sample - loss: 8.7473 - val_loss: 7.7930\n",
      "Epoch 327/2000\n",
      "39026/39026 [==============================] - 8s 211us/sample - loss: 8.7362 - val_loss: 7.7823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 328/2000\n",
      "39026/39026 [==============================] - 8s 209us/sample - loss: 8.7660 - val_loss: 7.8468\n",
      "Epoch 329/2000\n",
      "39026/39026 [==============================] - 8s 207us/sample - loss: 8.7611 - val_loss: 7.7845\n",
      "Epoch 330/2000\n",
      "39026/39026 [==============================] - 8s 207us/sample - loss: 8.7357 - val_loss: 7.7902\n",
      "Epoch 331/2000\n",
      "39026/39026 [==============================] - 8s 206us/sample - loss: 8.7276 - val_loss: 7.8091\n",
      "Epoch 332/2000\n",
      "39026/39026 [==============================] - 8s 205us/sample - loss: 8.7511 - val_loss: 7.8057\n",
      "Epoch 333/2000\n",
      "39026/39026 [==============================] - 8s 208us/sample - loss: 8.7800 - val_loss: 7.7944\n",
      "Epoch 334/2000\n",
      "39026/39026 [==============================] - 8s 207us/sample - loss: 8.8144 - val_loss: 7.8273\n",
      "Epoch 335/2000\n",
      "39026/39026 [==============================] - 8s 210us/sample - loss: 8.7313 - val_loss: 7.7813\n",
      "Epoch 336/2000\n",
      "39026/39026 [==============================] - 8s 212us/sample - loss: 8.7151 - val_loss: 7.7931\n",
      "Epoch 337/2000\n",
      "39026/39026 [==============================] - 8s 208us/sample - loss: 8.6910 - val_loss: 7.8014\n",
      "Epoch 338/2000\n",
      "39026/39026 [==============================] - 8s 214us/sample - loss: 8.6753 - val_loss: 7.8491\n",
      "Epoch 339/2000\n",
      "39026/39026 [==============================] - 9s 222us/sample - loss: 8.7137 - val_loss: 7.8788\n",
      "Epoch 340/2000\n",
      "39026/39026 [==============================] - 9s 220us/sample - loss: 8.6889 - val_loss: 7.8256\n",
      "Epoch 341/2000\n",
      "39026/39026 [==============================] - 8s 207us/sample - loss: 8.6787 - val_loss: 8.0152\n",
      "Epoch 342/2000\n",
      "39026/39026 [==============================] - 8s 209us/sample - loss: 8.6714 - val_loss: 7.8424\n",
      "Epoch 343/2000\n",
      "39026/39026 [==============================] - 8s 208us/sample - loss: 8.6784 - val_loss: 7.8291\n",
      "Epoch 344/2000\n",
      "39026/39026 [==============================] - 8s 205us/sample - loss: 8.7017 - val_loss: 7.8028\n",
      "Epoch 345/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.6783 - val_loss: 7.8471\n",
      "Epoch 346/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.6684 - val_loss: 7.9476\n",
      "Epoch 347/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.6553 - val_loss: 7.8566\n",
      "Epoch 348/2000\n",
      "39026/39026 [==============================] - 8s 207us/sample - loss: 8.6994 - val_loss: 7.8796\n",
      "Epoch 349/2000\n",
      "39026/39026 [==============================] - 8s 207us/sample - loss: 8.6068 - val_loss: 7.8262\n",
      "Epoch 370/2000\n",
      "39026/39026 [==============================] - 8s 203us/sample - loss: 8.6411 - val_loss: 7.8200\n",
      "Epoch 371/2000\n",
      "39026/39026 [==============================] - 8s 208us/sample - loss: 8.6020 - val_loss: 7.7989\n",
      "Epoch 372/2000\n",
      "39026/39026 [==============================] - 9s 231us/sample - loss: 8.6215 - val_loss: 7.8192\n",
      "Epoch 373/2000\n",
      "39026/39026 [==============================] - 9s 226us/sample - loss: 8.6225 - val_loss: 7.7623\n",
      "Epoch 374/2000\n",
      "39026/39026 [==============================] - 8s 211us/sample - loss: 8.5770 - val_loss: 7.7574\n",
      "Epoch 375/2000\n",
      "39026/39026 [==============================] - 8s 203us/sample - loss: 8.5956 - val_loss: 7.7833\n",
      "Epoch 376/2000\n",
      "39026/39026 [==============================] - 8s 214us/sample - loss: 8.6114 - val_loss: 7.6819\n",
      "Epoch 377/2000\n",
      "39026/39026 [==============================] - 8s 206us/sample - loss: 8.5482 - val_loss: 7.6631\n",
      "Epoch 378/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.6547 - val_loss: 7.6664\n",
      "Epoch 379/2000\n",
      "39026/39026 [==============================] - 8s 203us/sample - loss: 8.6208 - val_loss: 7.6679\n",
      "Epoch 380/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.6183 - val_loss: 7.6605\n",
      "Epoch 381/2000\n",
      "39026/39026 [==============================] - 8s 203us/sample - loss: 8.5960 - val_loss: 7.6630\n",
      "Epoch 382/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.6510 - val_loss: 7.6553\n",
      "Epoch 383/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.6274 - val_loss: 7.6523\n",
      "Epoch 384/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.5902 - val_loss: 7.6373\n",
      "Epoch 385/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.6629 - val_loss: 7.6644\n",
      "Epoch 386/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.5650 - val_loss: 7.6386\n",
      "Epoch 387/2000\n",
      "39026/39026 [==============================] - 8s 203us/sample - loss: 8.6242 - val_loss: 7.6425\n",
      "Epoch 388/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.6122 - val_loss: 7.6190\n",
      "Epoch 389/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.6438 - val_loss: 7.6318\n",
      "Epoch 390/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.6233 - val_loss: 7.6304\n",
      "Epoch 391/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.6162 - val_loss: 7.6294\n",
      "Epoch 392/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.6038 - val_loss: 7.6291\n",
      "Epoch 393/2000\n",
      "39026/39026 [==============================] - 8s 206us/sample - loss: 8.6277 - val_loss: 7.6182\n",
      "Epoch 394/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.6161 - val_loss: 7.6241\n",
      "Epoch 395/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.6258 - val_loss: 7.6198\n",
      "Epoch 396/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.6127 - val_loss: 7.6213\n",
      "Epoch 397/2000\n",
      "39026/39026 [==============================] - 8s 206us/sample - loss: 8.6162 - val_loss: 7.6245\n",
      "Epoch 398/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.6478 - val_loss: 7.6056\n",
      "Epoch 399/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.6023 - val_loss: 7.6166\n",
      "Epoch 400/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.6471 - val_loss: 7.6032\n",
      "Epoch 401/2000\n",
      "39026/39026 [==============================] - 8s 203us/sample - loss: 8.6263 - val_loss: 7.6136\n",
      "Epoch 402/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.6216 - val_loss: 7.6222\n",
      "Epoch 403/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.5820 - val_loss: 7.6175\n",
      "Epoch 404/2000\n",
      "39026/39026 [==============================] - 8s 203us/sample - loss: 8.5903 - val_loss: 7.6213\n",
      "Epoch 405/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.5905 - val_loss: 7.6072\n",
      "Epoch 406/2000\n",
      "39026/39026 [==============================] - 8s 202us/sample - loss: 8.5975 - val_loss: 7.6141\n",
      "Epoch 407/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.5825 - val_loss: 7.6175\n",
      "Epoch 408/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.5745 - val_loss: 7.6143\n",
      "Epoch 409/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.6076 - val_loss: 7.6084\n",
      "Epoch 410/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.5801 - val_loss: 7.6166\n",
      "Epoch 411/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.6273 - val_loss: 7.6096\n",
      "Epoch 412/2000\n",
      "39026/39026 [==============================] - 8s 206us/sample - loss: 8.6272 - val_loss: 7.6216\n",
      "Epoch 413/2000\n",
      "39026/39026 [==============================] - 8s 205us/sample - loss: 8.5856 - val_loss: 7.6089\n",
      "Epoch 414/2000\n",
      "39026/39026 [==============================] - 8s 205us/sample - loss: 8.5715 - val_loss: 7.6285\n",
      "Epoch 415/2000\n",
      "39026/39026 [==============================] - 8s 199us/sample - loss: 8.6069 - val_loss: 7.6201\n",
      "Epoch 416/2000\n",
      "39026/39026 [==============================] - 8s 199us/sample - loss: 8.5758 - val_loss: 7.6165\n",
      "Epoch 417/2000\n",
      "39026/39026 [==============================] - 8s 198us/sample - loss: 8.6120 - val_loss: 7.6408\n",
      "Epoch 418/2000\n",
      "39026/39026 [==============================] - 8s 198us/sample - loss: 8.5893 - val_loss: 7.6101\n",
      "Epoch 419/2000\n",
      "39026/39026 [==============================] - 8s 199us/sample - loss: 8.5684 - val_loss: 7.6014\n",
      "Epoch 420/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.6019 - val_loss: 7.6132\n",
      "Epoch 421/2000\n",
      "39026/39026 [==============================] - 8s 198us/sample - loss: 8.6324 - val_loss: 7.6183\n",
      "Epoch 422/2000\n",
      "39026/39026 [==============================] - 8s 197us/sample - loss: 8.6072 - val_loss: 7.6288\n",
      "Epoch 423/2000\n",
      "39026/39026 [==============================] - 8s 200us/sample - loss: 8.5658 - val_loss: 7.6042\n",
      "Epoch 424/2000\n",
      "39026/39026 [==============================] - 8s 198us/sample - loss: 8.6127 - val_loss: 7.6345\n",
      "Epoch 425/2000\n",
      "39026/39026 [==============================] - 8s 199us/sample - loss: 8.5942 - val_loss: 7.6267\n",
      "Epoch 426/2000\n",
      "39026/39026 [==============================] - 8s 197us/sample - loss: 8.5821 - val_loss: 7.6176\n",
      "Epoch 427/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.5606 - val_loss: 7.6104\n",
      "Epoch 428/2000\n",
      "39026/39026 [==============================] - 8s 204us/sample - loss: 8.5810 - val_loss: 7.6255\n",
      "Epoch 429/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.5808 - val_loss: 7.6096\n",
      "Epoch 430/2000\n",
      "39026/39026 [==============================] - 8s 201us/sample - loss: 8.5889 - val_loss: 7.5982\n",
      "Epoch 431/2000\n",
      "39026/39026 [==============================] - 8s 216us/sample - loss: 8.5780 - val_loss: 7.6009\n",
      "Epoch 432/2000\n",
      "39026/39026 [==============================] - 8s 216us/sample - loss: 8.6316 - val_loss: 7.6220\n",
      "Epoch 433/2000\n",
      "39026/39026 [==============================] - 10s 245us/sample - loss: 8.6138 - val_loss: 7.6233\n",
      "Epoch 434/2000\n",
      "39026/39026 [==============================] - 10s 252us/sample - loss: 8.5990 - val_loss: 7.6068\n",
      "Epoch 435/2000\n",
      "39026/39026 [==============================] - 9s 232us/sample - loss: 8.5587 - val_loss: 7.6000\n",
      "Epoch 436/2000\n",
      "39026/39026 [==============================] - 9s 241us/sample - loss: 8.6080 - val_loss: 7.6023\n",
      "Epoch 437/2000\n",
      "39026/39026 [==============================] - 9s 233us/sample - loss: 8.5734 - val_loss: 7.6118\n",
      "Epoch 438/2000\n",
      "39026/39026 [==============================] - 9s 222us/sample - loss: 8.5885 - val_loss: 7.6100\n",
      "Epoch 439/2000\n",
      "39026/39026 [==============================] - 9s 221us/sample - loss: 8.5808 - val_loss: 7.6010\n",
      "Epoch 440/2000\n",
      "39026/39026 [==============================] - 9s 220us/sample - loss: 8.6190 - val_loss: 7.5949\n",
      "Epoch 441/2000\n",
      "38912/39026 [============================>.] - ETA: 0s - loss: 8.6249Restoring model weights from the end of the best epoch.\n",
      "39026/39026 [==============================] - 8s 203us/sample - loss: 8.6251 - val_loss: 7.5991\n",
      "Epoch 00441: early stopping\n",
      "validation score for 1JHN is 2.0364189450267434 with NN\n",
      "\n",
      "[0]\tvalidation_0-mae:42.0936\n",
      "Will train until validation_0-mae hasn't improved in 32 rounds.\n",
      "[1]\tvalidation_0-mae:37.8777\n",
      "[2]\tvalidation_0-mae:34.0882\n",
      "[3]\tvalidation_0-mae:30.6749\n",
      "[4]\tvalidation_0-mae:27.6041\n",
      "[5]\tvalidation_0-mae:24.8402\n",
      "[6]\tvalidation_0-mae:22.3534\n",
      "[7]\tvalidation_0-mae:20.1164\n",
      "[8]\tvalidation_0-mae:18.1013\n",
      "[9]\tvalidation_0-mae:16.289\n",
      "[10]\tvalidation_0-mae:14.66\n",
      "[11]\tvalidation_0-mae:13.1934\n",
      "[12]\tvalidation_0-mae:11.8746\n",
      "[13]\tvalidation_0-mae:10.6861\n",
      "[14]\tvalidation_0-mae:9.61781\n",
      "[15]\tvalidation_0-mae:8.65653\n",
      "[16]\tvalidation_0-mae:7.79013\n",
      "[17]\tvalidation_0-mae:7.01131\n",
      "[18]\tvalidation_0-mae:6.3102\n",
      "[19]\tvalidation_0-mae:5.6784\n",
      "[20]\tvalidation_0-mae:5.10979\n",
      "[21]\tvalidation_0-mae:4.60017\n",
      "[22]\tvalidation_0-mae:4.14157\n",
      "[23]\tvalidation_0-mae:3.72882\n",
      "[24]\tvalidation_0-mae:3.35816\n",
      "[25]\tvalidation_0-mae:3.0262\n",
      "[26]\tvalidation_0-mae:2.72593\n",
      "[27]\tvalidation_0-mae:2.45745\n",
      "[28]\tvalidation_0-mae:2.21667\n",
      "[29]\tvalidation_0-mae:2.00306\n",
      "[30]\tvalidation_0-mae:1.8122\n",
      "[31]\tvalidation_0-mae:1.64321\n",
      "[32]\tvalidation_0-mae:1.49228\n",
      "[33]\tvalidation_0-mae:1.35917\n",
      "[34]\tvalidation_0-mae:1.24158\n",
      "[35]\tvalidation_0-mae:1.13789\n",
      "[36]\tvalidation_0-mae:1.04798\n",
      "[37]\tvalidation_0-mae:0.969002\n",
      "[38]\tvalidation_0-mae:0.901739\n",
      "[39]\tvalidation_0-mae:0.843299\n",
      "[40]\tvalidation_0-mae:0.793056\n",
      "[41]\tvalidation_0-mae:0.750569\n",
      "[42]\tvalidation_0-mae:0.713623\n",
      "[43]\tvalidation_0-mae:0.681568\n",
      "[44]\tvalidation_0-mae:0.654352\n",
      "[45]\tvalidation_0-mae:0.631245\n",
      "[46]\tvalidation_0-mae:0.612049\n",
      "[47]\tvalidation_0-mae:0.595944\n",
      "[48]\tvalidation_0-mae:0.581774\n",
      "[49]\tvalidation_0-mae:0.570338\n",
      "[50]\tvalidation_0-mae:0.560455\n",
      "[51]\tvalidation_0-mae:0.551667\n",
      "[52]\tvalidation_0-mae:0.544017\n",
      "[53]\tvalidation_0-mae:0.537696\n",
      "[54]\tvalidation_0-mae:0.531658\n",
      "[55]\tvalidation_0-mae:0.526176\n",
      "[56]\tvalidation_0-mae:0.520497\n",
      "[57]\tvalidation_0-mae:0.516305\n",
      "[58]\tvalidation_0-mae:0.512789\n",
      "[59]\tvalidation_0-mae:0.509095\n",
      "[60]\tvalidation_0-mae:0.505722\n",
      "[61]\tvalidation_0-mae:0.50122\n",
      "[62]\tvalidation_0-mae:0.498789\n",
      "[63]\tvalidation_0-mae:0.494839\n",
      "[64]\tvalidation_0-mae:0.492557\n",
      "[65]\tvalidation_0-mae:0.489784\n",
      "[66]\tvalidation_0-mae:0.488232\n",
      "[67]\tvalidation_0-mae:0.486194\n",
      "[68]\tvalidation_0-mae:0.484017\n",
      "[69]\tvalidation_0-mae:0.482883\n",
      "[70]\tvalidation_0-mae:0.480612\n",
      "[71]\tvalidation_0-mae:0.479829\n",
      "[72]\tvalidation_0-mae:0.478551\n",
      "[73]\tvalidation_0-mae:0.476559\n",
      "[74]\tvalidation_0-mae:0.475411\n",
      "[75]\tvalidation_0-mae:0.473605\n",
      "[76]\tvalidation_0-mae:0.471691\n",
      "[77]\tvalidation_0-mae:0.47105\n",
      "[78]\tvalidation_0-mae:0.469361\n",
      "[79]\tvalidation_0-mae:0.467648\n",
      "[80]\tvalidation_0-mae:0.467273\n",
      "[81]\tvalidation_0-mae:0.465099\n",
      "[82]\tvalidation_0-mae:0.463215\n",
      "[83]\tvalidation_0-mae:0.46189\n",
      "[84]\tvalidation_0-mae:0.460454\n",
      "[85]\tvalidation_0-mae:0.45987\n",
      "[86]\tvalidation_0-mae:0.458095\n",
      "[87]\tvalidation_0-mae:0.456772\n",
      "[88]\tvalidation_0-mae:0.455812\n",
      "[89]\tvalidation_0-mae:0.453815\n",
      "[90]\tvalidation_0-mae:0.452719\n",
      "[91]\tvalidation_0-mae:0.45129\n",
      "[92]\tvalidation_0-mae:0.450303\n",
      "[93]\tvalidation_0-mae:0.448984\n",
      "[94]\tvalidation_0-mae:0.447852\n",
      "[95]\tvalidation_0-mae:0.446874\n",
      "[96]\tvalidation_0-mae:0.445864\n",
      "[97]\tvalidation_0-mae:0.444281\n",
      "[98]\tvalidation_0-mae:0.443172\n",
      "[99]\tvalidation_0-mae:0.442907\n",
      "[100]\tvalidation_0-mae:0.442114\n",
      "[101]\tvalidation_0-mae:0.441465\n",
      "[102]\tvalidation_0-mae:0.440438\n",
      "[103]\tvalidation_0-mae:0.439414\n",
      "[104]\tvalidation_0-mae:0.438905\n",
      "[105]\tvalidation_0-mae:0.438338\n",
      "[106]\tvalidation_0-mae:0.43784\n",
      "[107]\tvalidation_0-mae:0.436648\n",
      "[108]\tvalidation_0-mae:0.435984\n",
      "[109]\tvalidation_0-mae:0.435219\n",
      "[110]\tvalidation_0-mae:0.434339\n",
      "[111]\tvalidation_0-mae:0.434039\n",
      "[112]\tvalidation_0-mae:0.433346\n",
      "[113]\tvalidation_0-mae:0.432381\n",
      "[114]\tvalidation_0-mae:0.431729\n",
      "[115]\tvalidation_0-mae:0.430873\n",
      "[116]\tvalidation_0-mae:0.429843\n",
      "[117]\tvalidation_0-mae:0.429071\n",
      "[118]\tvalidation_0-mae:0.428067\n",
      "[119]\tvalidation_0-mae:0.427227\n",
      "[120]\tvalidation_0-mae:0.426364\n",
      "[121]\tvalidation_0-mae:0.425848\n",
      "[122]\tvalidation_0-mae:0.424812\n",
      "[123]\tvalidation_0-mae:0.424377\n",
      "[124]\tvalidation_0-mae:0.423261\n",
      "[125]\tvalidation_0-mae:0.423\n",
      "[126]\tvalidation_0-mae:0.422174\n",
      "[127]\tvalidation_0-mae:0.421463\n",
      "[128]\tvalidation_0-mae:0.420994\n",
      "[129]\tvalidation_0-mae:0.420079\n",
      "[130]\tvalidation_0-mae:0.419205\n",
      "[131]\tvalidation_0-mae:0.418297\n",
      "[132]\tvalidation_0-mae:0.41776\n",
      "[133]\tvalidation_0-mae:0.417482\n",
      "[134]\tvalidation_0-mae:0.416348\n",
      "[135]\tvalidation_0-mae:0.415663\n",
      "[136]\tvalidation_0-mae:0.415034\n",
      "[137]\tvalidation_0-mae:0.414634\n",
      "[138]\tvalidation_0-mae:0.414379\n",
      "[139]\tvalidation_0-mae:0.413527\n",
      "[140]\tvalidation_0-mae:0.412927\n",
      "[141]\tvalidation_0-mae:0.41265\n",
      "[142]\tvalidation_0-mae:0.411981\n",
      "[143]\tvalidation_0-mae:0.411409\n",
      "[144]\tvalidation_0-mae:0.410674\n",
      "[145]\tvalidation_0-mae:0.410133\n",
      "[146]\tvalidation_0-mae:0.409534\n",
      "[147]\tvalidation_0-mae:0.408803\n",
      "[148]\tvalidation_0-mae:0.408415\n",
      "[149]\tvalidation_0-mae:0.407843\n",
      "[150]\tvalidation_0-mae:0.407518\n",
      "[151]\tvalidation_0-mae:0.407178\n",
      "[152]\tvalidation_0-mae:0.406652\n",
      "[153]\tvalidation_0-mae:0.406352\n",
      "[154]\tvalidation_0-mae:0.405948\n",
      "[155]\tvalidation_0-mae:0.405741\n",
      "[156]\tvalidation_0-mae:0.405369\n",
      "[157]\tvalidation_0-mae:0.404884\n",
      "[158]\tvalidation_0-mae:0.404537\n",
      "[159]\tvalidation_0-mae:0.404345\n",
      "[160]\tvalidation_0-mae:0.403736\n",
      "[161]\tvalidation_0-mae:0.403242\n",
      "[162]\tvalidation_0-mae:0.402962\n",
      "[163]\tvalidation_0-mae:0.402678\n",
      "[164]\tvalidation_0-mae:0.402491\n",
      "[165]\tvalidation_0-mae:0.402047\n",
      "[166]\tvalidation_0-mae:0.401648\n",
      "[167]\tvalidation_0-mae:0.401117\n",
      "[168]\tvalidation_0-mae:0.400245\n",
      "[169]\tvalidation_0-mae:0.399377\n",
      "[170]\tvalidation_0-mae:0.398705\n",
      "[171]\tvalidation_0-mae:0.398333\n",
      "[172]\tvalidation_0-mae:0.397874\n",
      "[173]\tvalidation_0-mae:0.397512\n",
      "[174]\tvalidation_0-mae:0.396881\n",
      "[175]\tvalidation_0-mae:0.396664\n",
      "[176]\tvalidation_0-mae:0.396221\n",
      "[177]\tvalidation_0-mae:0.395789\n",
      "[178]\tvalidation_0-mae:0.3955\n",
      "[179]\tvalidation_0-mae:0.395027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[180]\tvalidation_0-mae:0.394893\n",
      "[181]\tvalidation_0-mae:0.394737\n",
      "[182]\tvalidation_0-mae:0.394524\n",
      "[183]\tvalidation_0-mae:0.394049\n",
      "[184]\tvalidation_0-mae:0.393686\n",
      "[185]\tvalidation_0-mae:0.393393\n",
      "[186]\tvalidation_0-mae:0.39308\n",
      "[187]\tvalidation_0-mae:0.392782\n",
      "[188]\tvalidation_0-mae:0.392334\n",
      "[189]\tvalidation_0-mae:0.391961\n",
      "[190]\tvalidation_0-mae:0.391646\n",
      "[191]\tvalidation_0-mae:0.391494\n",
      "[192]\tvalidation_0-mae:0.390922\n",
      "[193]\tvalidation_0-mae:0.390658\n",
      "[194]\tvalidation_0-mae:0.390204\n",
      "[195]\tvalidation_0-mae:0.389987\n",
      "[196]\tvalidation_0-mae:0.389661\n",
      "[197]\tvalidation_0-mae:0.389522\n",
      "[198]\tvalidation_0-mae:0.389206\n",
      "[199]\tvalidation_0-mae:0.389037\n",
      "[200]\tvalidation_0-mae:0.388732\n",
      "[201]\tvalidation_0-mae:0.388534\n",
      "[202]\tvalidation_0-mae:0.388201\n",
      "[203]\tvalidation_0-mae:0.387934\n",
      "[204]\tvalidation_0-mae:0.387742\n",
      "[205]\tvalidation_0-mae:0.387429\n",
      "[206]\tvalidation_0-mae:0.387164\n",
      "[207]\tvalidation_0-mae:0.387018\n",
      "[208]\tvalidation_0-mae:0.386857\n",
      "[209]\tvalidation_0-mae:0.386701\n",
      "[210]\tvalidation_0-mae:0.38631\n",
      "[211]\tvalidation_0-mae:0.386021\n",
      "[212]\tvalidation_0-mae:0.385871\n",
      "[213]\tvalidation_0-mae:0.385648\n",
      "[214]\tvalidation_0-mae:0.385268\n",
      "[215]\tvalidation_0-mae:0.385067\n",
      "[216]\tvalidation_0-mae:0.384704\n",
      "[217]\tvalidation_0-mae:0.38429\n",
      "[218]\tvalidation_0-mae:0.38396\n",
      "[219]\tvalidation_0-mae:0.383711\n",
      "[220]\tvalidation_0-mae:0.383401\n",
      "[221]\tvalidation_0-mae:0.383212\n",
      "[222]\tvalidation_0-mae:0.382831\n",
      "[223]\tvalidation_0-mae:0.382477\n",
      "[224]\tvalidation_0-mae:0.382243\n",
      "[225]\tvalidation_0-mae:0.382154\n",
      "[226]\tvalidation_0-mae:0.382059\n",
      "[227]\tvalidation_0-mae:0.381924\n",
      "[228]\tvalidation_0-mae:0.381739\n",
      "[229]\tvalidation_0-mae:0.381414\n",
      "[230]\tvalidation_0-mae:0.381275\n",
      "[231]\tvalidation_0-mae:0.381142\n",
      "[232]\tvalidation_0-mae:0.380836\n",
      "[233]\tvalidation_0-mae:0.380685\n",
      "[234]\tvalidation_0-mae:0.380481\n",
      "[235]\tvalidation_0-mae:0.380057\n",
      "[236]\tvalidation_0-mae:0.379831\n",
      "[237]\tvalidation_0-mae:0.379561\n",
      "[238]\tvalidation_0-mae:0.379436\n",
      "[239]\tvalidation_0-mae:0.379154\n",
      "[240]\tvalidation_0-mae:0.378949\n",
      "[241]\tvalidation_0-mae:0.37881\n",
      "[242]\tvalidation_0-mae:0.378648\n",
      "[243]\tvalidation_0-mae:0.378523\n",
      "[244]\tvalidation_0-mae:0.378343\n",
      "[245]\tvalidation_0-mae:0.378027\n",
      "[246]\tvalidation_0-mae:0.377971\n",
      "[247]\tvalidation_0-mae:0.377802\n",
      "[248]\tvalidation_0-mae:0.377495\n",
      "[249]\tvalidation_0-mae:0.377344\n",
      "[250]\tvalidation_0-mae:0.377216\n",
      "[251]\tvalidation_0-mae:0.37703\n",
      "[252]\tvalidation_0-mae:0.376782\n",
      "[253]\tvalidation_0-mae:0.376495\n",
      "[254]\tvalidation_0-mae:0.376289\n",
      "[255]\tvalidation_0-mae:0.376025\n",
      "[256]\tvalidation_0-mae:0.375799\n",
      "[257]\tvalidation_0-mae:0.375593\n",
      "[258]\tvalidation_0-mae:0.375527\n",
      "[259]\tvalidation_0-mae:0.375531\n",
      "[260]\tvalidation_0-mae:0.375481\n",
      "[261]\tvalidation_0-mae:0.375274\n",
      "[262]\tvalidation_0-mae:0.375079\n",
      "[263]\tvalidation_0-mae:0.374969\n",
      "[264]\tvalidation_0-mae:0.374839\n",
      "[265]\tvalidation_0-mae:0.374744\n",
      "[266]\tvalidation_0-mae:0.374701\n",
      "[267]\tvalidation_0-mae:0.374513\n",
      "[268]\tvalidation_0-mae:0.374311\n",
      "[269]\tvalidation_0-mae:0.374197\n",
      "[270]\tvalidation_0-mae:0.37406\n",
      "[271]\tvalidation_0-mae:0.373899\n",
      "[272]\tvalidation_0-mae:0.373785\n",
      "[273]\tvalidation_0-mae:0.373703\n",
      "[274]\tvalidation_0-mae:0.373515\n",
      "[275]\tvalidation_0-mae:0.373384\n",
      "[276]\tvalidation_0-mae:0.373201\n",
      "[277]\tvalidation_0-mae:0.373064\n",
      "[278]\tvalidation_0-mae:0.372959\n",
      "[279]\tvalidation_0-mae:0.372803\n",
      "[280]\tvalidation_0-mae:0.372723\n",
      "[281]\tvalidation_0-mae:0.37263\n",
      "[282]\tvalidation_0-mae:0.372498\n",
      "[283]\tvalidation_0-mae:0.372273\n",
      "[284]\tvalidation_0-mae:0.372072\n",
      "[285]\tvalidation_0-mae:0.372021\n",
      "[286]\tvalidation_0-mae:0.371916\n",
      "[287]\tvalidation_0-mae:0.371652\n",
      "[288]\tvalidation_0-mae:0.371511\n",
      "[289]\tvalidation_0-mae:0.371408\n",
      "[290]\tvalidation_0-mae:0.371337\n",
      "[291]\tvalidation_0-mae:0.371141\n",
      "[292]\tvalidation_0-mae:0.371049\n",
      "[293]\tvalidation_0-mae:0.371005\n",
      "[294]\tvalidation_0-mae:0.370871\n",
      "[295]\tvalidation_0-mae:0.370729\n",
      "[296]\tvalidation_0-mae:0.370594\n",
      "[297]\tvalidation_0-mae:0.370487\n",
      "[298]\tvalidation_0-mae:0.370254\n",
      "[299]\tvalidation_0-mae:0.370174\n",
      "[300]\tvalidation_0-mae:0.370078\n",
      "[301]\tvalidation_0-mae:0.36999\n",
      "[302]\tvalidation_0-mae:0.36979\n",
      "[303]\tvalidation_0-mae:0.369636\n",
      "[304]\tvalidation_0-mae:0.369489\n",
      "[305]\tvalidation_0-mae:0.369285\n",
      "[306]\tvalidation_0-mae:0.36923\n",
      "[307]\tvalidation_0-mae:0.369159\n",
      "[308]\tvalidation_0-mae:0.369055\n",
      "[309]\tvalidation_0-mae:0.368881\n",
      "[310]\tvalidation_0-mae:0.368718\n",
      "[311]\tvalidation_0-mae:0.368656\n",
      "[312]\tvalidation_0-mae:0.368499\n",
      "[313]\tvalidation_0-mae:0.368335\n",
      "[314]\tvalidation_0-mae:0.368225\n",
      "[315]\tvalidation_0-mae:0.368157\n",
      "[316]\tvalidation_0-mae:0.368086\n",
      "[317]\tvalidation_0-mae:0.367866\n",
      "[318]\tvalidation_0-mae:0.36774\n",
      "[319]\tvalidation_0-mae:0.367614\n",
      "[320]\tvalidation_0-mae:0.367506\n",
      "[321]\tvalidation_0-mae:0.367302\n",
      "[322]\tvalidation_0-mae:0.367202\n",
      "[323]\tvalidation_0-mae:0.367112\n",
      "[324]\tvalidation_0-mae:0.366976\n",
      "[325]\tvalidation_0-mae:0.366837\n",
      "[326]\tvalidation_0-mae:0.366778\n",
      "[327]\tvalidation_0-mae:0.366712\n",
      "[328]\tvalidation_0-mae:0.366607\n",
      "[329]\tvalidation_0-mae:0.366551\n",
      "[330]\tvalidation_0-mae:0.366424\n",
      "[331]\tvalidation_0-mae:0.366299\n",
      "[332]\tvalidation_0-mae:0.366199\n",
      "[333]\tvalidation_0-mae:0.366133\n",
      "[334]\tvalidation_0-mae:0.366084\n",
      "[335]\tvalidation_0-mae:0.365966\n",
      "[336]\tvalidation_0-mae:0.365898\n",
      "[337]\tvalidation_0-mae:0.365801\n",
      "[338]\tvalidation_0-mae:0.365652\n",
      "[339]\tvalidation_0-mae:0.365554\n",
      "[340]\tvalidation_0-mae:0.365394\n",
      "[341]\tvalidation_0-mae:0.365295\n",
      "[342]\tvalidation_0-mae:0.365246\n",
      "[343]\tvalidation_0-mae:0.365183\n",
      "[344]\tvalidation_0-mae:0.365083\n",
      "[345]\tvalidation_0-mae:0.365013\n",
      "[346]\tvalidation_0-mae:0.364934\n",
      "[347]\tvalidation_0-mae:0.364877\n",
      "[348]\tvalidation_0-mae:0.364758\n",
      "[349]\tvalidation_0-mae:0.364607\n",
      "[350]\tvalidation_0-mae:0.364451\n",
      "[351]\tvalidation_0-mae:0.364409\n",
      "[352]\tvalidation_0-mae:0.364326\n",
      "[353]\tvalidation_0-mae:0.364202\n",
      "[354]\tvalidation_0-mae:0.364116\n",
      "[355]\tvalidation_0-mae:0.364054\n",
      "[356]\tvalidation_0-mae:0.36398\n",
      "[357]\tvalidation_0-mae:0.363924\n",
      "[358]\tvalidation_0-mae:0.363837\n",
      "[359]\tvalidation_0-mae:0.363725\n",
      "[360]\tvalidation_0-mae:0.363689\n",
      "[361]\tvalidation_0-mae:0.363645\n",
      "[362]\tvalidation_0-mae:0.363601\n",
      "[363]\tvalidation_0-mae:0.363479\n",
      "[364]\tvalidation_0-mae:0.3633\n",
      "[365]\tvalidation_0-mae:0.363255\n",
      "[366]\tvalidation_0-mae:0.363201\n",
      "[367]\tvalidation_0-mae:0.363109\n",
      "[368]\tvalidation_0-mae:0.362936\n",
      "[369]\tvalidation_0-mae:0.362909\n",
      "[370]\tvalidation_0-mae:0.362847\n",
      "[371]\tvalidation_0-mae:0.36279\n",
      "[372]\tvalidation_0-mae:0.362792\n",
      "[373]\tvalidation_0-mae:0.36276\n",
      "[374]\tvalidation_0-mae:0.362656\n",
      "[375]\tvalidation_0-mae:0.362594\n",
      "[376]\tvalidation_0-mae:0.362553\n",
      "[377]\tvalidation_0-mae:0.362452\n",
      "[378]\tvalidation_0-mae:0.362362\n",
      "[379]\tvalidation_0-mae:0.362291\n",
      "[380]\tvalidation_0-mae:0.362265\n",
      "[381]\tvalidation_0-mae:0.362227\n",
      "[382]\tvalidation_0-mae:0.362155\n",
      "[383]\tvalidation_0-mae:0.362138\n",
      "[384]\tvalidation_0-mae:0.36207\n",
      "[385]\tvalidation_0-mae:0.361972\n",
      "[386]\tvalidation_0-mae:0.361946\n",
      "[387]\tvalidation_0-mae:0.361852\n",
      "[388]\tvalidation_0-mae:0.361765\n",
      "[389]\tvalidation_0-mae:0.361724\n",
      "[390]\tvalidation_0-mae:0.361611\n",
      "[391]\tvalidation_0-mae:0.361512\n",
      "[392]\tvalidation_0-mae:0.361441\n",
      "[393]\tvalidation_0-mae:0.3614\n",
      "[394]\tvalidation_0-mae:0.361272\n",
      "[395]\tvalidation_0-mae:0.361194\n",
      "[396]\tvalidation_0-mae:0.361097\n",
      "[397]\tvalidation_0-mae:0.360949\n",
      "[398]\tvalidation_0-mae:0.360903\n",
      "[399]\tvalidation_0-mae:0.36085\n",
      "[400]\tvalidation_0-mae:0.360764\n",
      "[401]\tvalidation_0-mae:0.360685\n",
      "[402]\tvalidation_0-mae:0.360614\n",
      "[403]\tvalidation_0-mae:0.360558\n",
      "[404]\tvalidation_0-mae:0.360437\n",
      "[405]\tvalidation_0-mae:0.360391\n",
      "[406]\tvalidation_0-mae:0.360342\n",
      "[407]\tvalidation_0-mae:0.360289\n",
      "[408]\tvalidation_0-mae:0.360251\n",
      "[409]\tvalidation_0-mae:0.360214\n",
      "[410]\tvalidation_0-mae:0.360127\n",
      "[411]\tvalidation_0-mae:0.36005\n",
      "[412]\tvalidation_0-mae:0.359991\n",
      "[413]\tvalidation_0-mae:0.359833\n",
      "[414]\tvalidation_0-mae:0.359741\n",
      "[415]\tvalidation_0-mae:0.359736\n",
      "[416]\tvalidation_0-mae:0.359544\n",
      "[417]\tvalidation_0-mae:0.359448\n",
      "[418]\tvalidation_0-mae:0.359398\n",
      "[419]\tvalidation_0-mae:0.359393\n",
      "[420]\tvalidation_0-mae:0.359312\n",
      "[421]\tvalidation_0-mae:0.359234\n",
      "[422]\tvalidation_0-mae:0.359164\n",
      "[423]\tvalidation_0-mae:0.35912\n",
      "[424]\tvalidation_0-mae:0.359068\n",
      "[425]\tvalidation_0-mae:0.358988\n",
      "[426]\tvalidation_0-mae:0.358942\n",
      "[427]\tvalidation_0-mae:0.358869\n",
      "[428]\tvalidation_0-mae:0.358793\n",
      "[429]\tvalidation_0-mae:0.358714\n",
      "[430]\tvalidation_0-mae:0.358612\n",
      "[431]\tvalidation_0-mae:0.358491\n",
      "[432]\tvalidation_0-mae:0.358406\n",
      "[433]\tvalidation_0-mae:0.358332\n",
      "[434]\tvalidation_0-mae:0.358315\n",
      "[435]\tvalidation_0-mae:0.358294\n",
      "[436]\tvalidation_0-mae:0.358205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[437]\tvalidation_0-mae:0.358168\n",
      "[438]\tvalidation_0-mae:0.358166\n",
      "[439]\tvalidation_0-mae:0.358131\n",
      "[440]\tvalidation_0-mae:0.358035\n",
      "[441]\tvalidation_0-mae:0.357996\n",
      "[442]\tvalidation_0-mae:0.357903\n",
      "[443]\tvalidation_0-mae:0.357818\n",
      "[444]\tvalidation_0-mae:0.357756\n",
      "[445]\tvalidation_0-mae:0.357668\n",
      "[446]\tvalidation_0-mae:0.357596\n",
      "[447]\tvalidation_0-mae:0.357583\n",
      "[448]\tvalidation_0-mae:0.357561\n",
      "[449]\tvalidation_0-mae:0.357509\n",
      "[450]\tvalidation_0-mae:0.357442\n",
      "[451]\tvalidation_0-mae:0.357344\n",
      "[452]\tvalidation_0-mae:0.357245\n",
      "[453]\tvalidation_0-mae:0.357189\n",
      "[454]\tvalidation_0-mae:0.357101\n",
      "[455]\tvalidation_0-mae:0.357009\n",
      "[456]\tvalidation_0-mae:0.356949\n",
      "[457]\tvalidation_0-mae:0.356851\n",
      "[458]\tvalidation_0-mae:0.356795\n",
      "[459]\tvalidation_0-mae:0.356722\n",
      "[460]\tvalidation_0-mae:0.356702\n",
      "[461]\tvalidation_0-mae:0.356625\n",
      "[462]\tvalidation_0-mae:0.356638\n",
      "[463]\tvalidation_0-mae:0.356574\n",
      "[464]\tvalidation_0-mae:0.356503\n",
      "[465]\tvalidation_0-mae:0.356472\n",
      "[466]\tvalidation_0-mae:0.356442\n",
      "[467]\tvalidation_0-mae:0.356363\n",
      "[468]\tvalidation_0-mae:0.356314\n",
      "[469]\tvalidation_0-mae:0.356276\n",
      "[470]\tvalidation_0-mae:0.356213\n",
      "[471]\tvalidation_0-mae:0.356161\n",
      "[472]\tvalidation_0-mae:0.35614\n",
      "[473]\tvalidation_0-mae:0.35613\n",
      "[474]\tvalidation_0-mae:0.356086\n",
      "[475]\tvalidation_0-mae:0.356044\n",
      "[476]\tvalidation_0-mae:0.356004\n",
      "[477]\tvalidation_0-mae:0.355966\n",
      "[478]\tvalidation_0-mae:0.355934\n",
      "[479]\tvalidation_0-mae:0.355964\n",
      "[480]\tvalidation_0-mae:0.355918\n",
      "[481]\tvalidation_0-mae:0.355858\n",
      "[482]\tvalidation_0-mae:0.355838\n",
      "[483]\tvalidation_0-mae:0.355825\n",
      "[484]\tvalidation_0-mae:0.355775\n",
      "[485]\tvalidation_0-mae:0.355774\n",
      "[486]\tvalidation_0-mae:0.35569\n",
      "[487]\tvalidation_0-mae:0.355668\n",
      "[488]\tvalidation_0-mae:0.355603\n",
      "[489]\tvalidation_0-mae:0.355582\n",
      "[490]\tvalidation_0-mae:0.355594\n",
      "[491]\tvalidation_0-mae:0.355543\n",
      "[492]\tvalidation_0-mae:0.355506\n",
      "[493]\tvalidation_0-mae:0.355483\n",
      "[494]\tvalidation_0-mae:0.355438\n",
      "[495]\tvalidation_0-mae:0.355386\n",
      "[496]\tvalidation_0-mae:0.355324\n",
      "[497]\tvalidation_0-mae:0.355313\n",
      "[498]\tvalidation_0-mae:0.355265\n",
      "[499]\tvalidation_0-mae:0.355212\n",
      "[500]\tvalidation_0-mae:0.355163\n",
      "[501]\tvalidation_0-mae:0.355073\n",
      "[502]\tvalidation_0-mae:0.355042\n",
      "[503]\tvalidation_0-mae:0.355003\n",
      "[504]\tvalidation_0-mae:0.354957\n",
      "[505]\tvalidation_0-mae:0.35494\n",
      "[506]\tvalidation_0-mae:0.354888\n",
      "[507]\tvalidation_0-mae:0.354871\n",
      "[508]\tvalidation_0-mae:0.354858\n",
      "[509]\tvalidation_0-mae:0.354816\n",
      "[510]\tvalidation_0-mae:0.354775\n",
      "[511]\tvalidation_0-mae:0.354744\n",
      "[512]\tvalidation_0-mae:0.354746\n",
      "[513]\tvalidation_0-mae:0.354749\n",
      "[514]\tvalidation_0-mae:0.354715\n",
      "[515]\tvalidation_0-mae:0.354686\n",
      "[516]\tvalidation_0-mae:0.354636\n",
      "[517]\tvalidation_0-mae:0.3546\n",
      "[518]\tvalidation_0-mae:0.35458\n",
      "[519]\tvalidation_0-mae:0.354539\n",
      "[520]\tvalidation_0-mae:0.354533\n",
      "[521]\tvalidation_0-mae:0.354492\n",
      "[522]\tvalidation_0-mae:0.354465\n",
      "[523]\tvalidation_0-mae:0.354412\n",
      "[524]\tvalidation_0-mae:0.354395\n",
      "[525]\tvalidation_0-mae:0.354335\n",
      "[526]\tvalidation_0-mae:0.354272\n",
      "[527]\tvalidation_0-mae:0.354269\n",
      "[528]\tvalidation_0-mae:0.354248\n",
      "[529]\tvalidation_0-mae:0.354222\n",
      "[530]\tvalidation_0-mae:0.354157\n",
      "[531]\tvalidation_0-mae:0.354116\n",
      "[532]\tvalidation_0-mae:0.35408\n",
      "[533]\tvalidation_0-mae:0.354075\n",
      "[534]\tvalidation_0-mae:0.35404\n",
      "[535]\tvalidation_0-mae:0.354038\n",
      "[536]\tvalidation_0-mae:0.353995\n",
      "[537]\tvalidation_0-mae:0.353957\n",
      "[538]\tvalidation_0-mae:0.35394\n",
      "[539]\tvalidation_0-mae:0.353922\n",
      "[540]\tvalidation_0-mae:0.353891\n",
      "[541]\tvalidation_0-mae:0.353816\n",
      "[542]\tvalidation_0-mae:0.353765\n",
      "[543]\tvalidation_0-mae:0.353726\n",
      "[544]\tvalidation_0-mae:0.353694\n",
      "[545]\tvalidation_0-mae:0.353643\n",
      "[546]\tvalidation_0-mae:0.353631\n",
      "[547]\tvalidation_0-mae:0.35362\n",
      "[548]\tvalidation_0-mae:0.353594\n",
      "[549]\tvalidation_0-mae:0.353543\n",
      "[550]\tvalidation_0-mae:0.353498\n",
      "[551]\tvalidation_0-mae:0.353462\n",
      "[552]\tvalidation_0-mae:0.35346\n",
      "[553]\tvalidation_0-mae:0.353407\n",
      "[554]\tvalidation_0-mae:0.353374\n",
      "[555]\tvalidation_0-mae:0.353348\n",
      "[556]\tvalidation_0-mae:0.353341\n",
      "[557]\tvalidation_0-mae:0.353265\n",
      "[558]\tvalidation_0-mae:0.353226\n",
      "[559]\tvalidation_0-mae:0.353183\n",
      "[560]\tvalidation_0-mae:0.353134\n",
      "[561]\tvalidation_0-mae:0.353113\n",
      "[562]\tvalidation_0-mae:0.353054\n",
      "[563]\tvalidation_0-mae:0.353044\n",
      "[564]\tvalidation_0-mae:0.353003\n",
      "[565]\tvalidation_0-mae:0.352974\n",
      "[566]\tvalidation_0-mae:0.352958\n",
      "[567]\tvalidation_0-mae:0.352949\n",
      "[568]\tvalidation_0-mae:0.352912\n",
      "[569]\tvalidation_0-mae:0.352876\n",
      "[570]\tvalidation_0-mae:0.352835\n",
      "[571]\tvalidation_0-mae:0.352814\n",
      "[572]\tvalidation_0-mae:0.352789\n",
      "[573]\tvalidation_0-mae:0.352753\n",
      "[574]\tvalidation_0-mae:0.352748\n",
      "[575]\tvalidation_0-mae:0.35271\n",
      "[576]\tvalidation_0-mae:0.352626\n",
      "[577]\tvalidation_0-mae:0.352599\n",
      "[578]\tvalidation_0-mae:0.352593\n",
      "[579]\tvalidation_0-mae:0.352577\n",
      "[580]\tvalidation_0-mae:0.352553\n",
      "[581]\tvalidation_0-mae:0.352529\n",
      "[582]\tvalidation_0-mae:0.352506\n",
      "[583]\tvalidation_0-mae:0.352482\n",
      "[584]\tvalidation_0-mae:0.352414\n",
      "[585]\tvalidation_0-mae:0.352383\n",
      "[586]\tvalidation_0-mae:0.352382\n",
      "[587]\tvalidation_0-mae:0.352333\n",
      "[588]\tvalidation_0-mae:0.352313\n",
      "[589]\tvalidation_0-mae:0.352295\n",
      "[590]\tvalidation_0-mae:0.352254\n",
      "[591]\tvalidation_0-mae:0.352243\n",
      "[592]\tvalidation_0-mae:0.352217\n",
      "[593]\tvalidation_0-mae:0.3522\n",
      "[594]\tvalidation_0-mae:0.352151\n",
      "[595]\tvalidation_0-mae:0.352131\n",
      "[596]\tvalidation_0-mae:0.352114\n",
      "[597]\tvalidation_0-mae:0.352088\n",
      "[598]\tvalidation_0-mae:0.352051\n",
      "[599]\tvalidation_0-mae:0.352002\n",
      "[600]\tvalidation_0-mae:0.351988\n",
      "[601]\tvalidation_0-mae:0.351974\n",
      "[602]\tvalidation_0-mae:0.351935\n",
      "[603]\tvalidation_0-mae:0.351915\n",
      "[604]\tvalidation_0-mae:0.351897\n",
      "[605]\tvalidation_0-mae:0.35188\n",
      "[606]\tvalidation_0-mae:0.351866\n",
      "[607]\tvalidation_0-mae:0.351856\n",
      "[608]\tvalidation_0-mae:0.351847\n",
      "[609]\tvalidation_0-mae:0.351853\n",
      "[610]\tvalidation_0-mae:0.351822\n",
      "[611]\tvalidation_0-mae:0.351809\n",
      "[612]\tvalidation_0-mae:0.351789\n",
      "[613]\tvalidation_0-mae:0.351763\n",
      "[614]\tvalidation_0-mae:0.351727\n",
      "[615]\tvalidation_0-mae:0.351696\n",
      "[616]\tvalidation_0-mae:0.351676\n",
      "[617]\tvalidation_0-mae:0.351668\n",
      "[618]\tvalidation_0-mae:0.351646\n",
      "[619]\tvalidation_0-mae:0.351612\n",
      "[620]\tvalidation_0-mae:0.351605\n",
      "[621]\tvalidation_0-mae:0.351574\n",
      "[622]\tvalidation_0-mae:0.35153\n",
      "[623]\tvalidation_0-mae:0.351482\n",
      "[624]\tvalidation_0-mae:0.351474\n",
      "[625]\tvalidation_0-mae:0.351442\n",
      "[626]\tvalidation_0-mae:0.351414\n",
      "[627]\tvalidation_0-mae:0.351392\n",
      "[628]\tvalidation_0-mae:0.351371\n",
      "[629]\tvalidation_0-mae:0.351355\n",
      "[630]\tvalidation_0-mae:0.351323\n",
      "[631]\tvalidation_0-mae:0.351305\n",
      "[632]\tvalidation_0-mae:0.351276\n",
      "[633]\tvalidation_0-mae:0.351264\n",
      "[634]\tvalidation_0-mae:0.351253\n",
      "[635]\tvalidation_0-mae:0.351214\n",
      "[636]\tvalidation_0-mae:0.351174\n",
      "[637]\tvalidation_0-mae:0.351112\n",
      "[638]\tvalidation_0-mae:0.351115\n",
      "[639]\tvalidation_0-mae:0.351092\n",
      "[640]\tvalidation_0-mae:0.351076\n",
      "[641]\tvalidation_0-mae:0.35105\n",
      "[642]\tvalidation_0-mae:0.351022\n",
      "[643]\tvalidation_0-mae:0.351\n",
      "[644]\tvalidation_0-mae:0.350989\n",
      "[645]\tvalidation_0-mae:0.350986\n",
      "[646]\tvalidation_0-mae:0.35097\n",
      "[647]\tvalidation_0-mae:0.350917\n",
      "[648]\tvalidation_0-mae:0.350879\n",
      "[649]\tvalidation_0-mae:0.350877\n",
      "[650]\tvalidation_0-mae:0.350867\n",
      "[651]\tvalidation_0-mae:0.350841\n",
      "[652]\tvalidation_0-mae:0.350818\n",
      "[653]\tvalidation_0-mae:0.350761\n",
      "[654]\tvalidation_0-mae:0.350742\n",
      "[655]\tvalidation_0-mae:0.350693\n",
      "[656]\tvalidation_0-mae:0.350681\n",
      "[657]\tvalidation_0-mae:0.350618\n",
      "[658]\tvalidation_0-mae:0.350609\n",
      "[659]\tvalidation_0-mae:0.350572\n",
      "[660]\tvalidation_0-mae:0.350544\n",
      "[661]\tvalidation_0-mae:0.350528\n",
      "[662]\tvalidation_0-mae:0.350535\n",
      "[663]\tvalidation_0-mae:0.350482\n",
      "[664]\tvalidation_0-mae:0.350449\n",
      "[665]\tvalidation_0-mae:0.35039\n",
      "[666]\tvalidation_0-mae:0.35038\n",
      "[667]\tvalidation_0-mae:0.350355\n",
      "[668]\tvalidation_0-mae:0.350333\n",
      "[669]\tvalidation_0-mae:0.350297\n",
      "[670]\tvalidation_0-mae:0.350266\n",
      "[671]\tvalidation_0-mae:0.350255\n",
      "[672]\tvalidation_0-mae:0.350218\n",
      "[673]\tvalidation_0-mae:0.350195\n",
      "[674]\tvalidation_0-mae:0.350165\n",
      "[675]\tvalidation_0-mae:0.350144\n",
      "[676]\tvalidation_0-mae:0.350108\n",
      "[677]\tvalidation_0-mae:0.350062\n",
      "[678]\tvalidation_0-mae:0.350055\n",
      "[679]\tvalidation_0-mae:0.350035\n",
      "[680]\tvalidation_0-mae:0.349997\n",
      "[681]\tvalidation_0-mae:0.349958\n",
      "[682]\tvalidation_0-mae:0.349948\n",
      "[683]\tvalidation_0-mae:0.349934\n",
      "[684]\tvalidation_0-mae:0.349924\n",
      "[685]\tvalidation_0-mae:0.349887\n",
      "[686]\tvalidation_0-mae:0.349872\n",
      "[687]\tvalidation_0-mae:0.349841\n",
      "[688]\tvalidation_0-mae:0.349822\n",
      "[689]\tvalidation_0-mae:0.349824\n",
      "[690]\tvalidation_0-mae:0.349803\n",
      "[691]\tvalidation_0-mae:0.349795\n",
      "[692]\tvalidation_0-mae:0.349762\n",
      "[693]\tvalidation_0-mae:0.349741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[694]\tvalidation_0-mae:0.349733\n",
      "[695]\tvalidation_0-mae:0.349722\n",
      "[696]\tvalidation_0-mae:0.349701\n",
      "[697]\tvalidation_0-mae:0.349656\n",
      "[698]\tvalidation_0-mae:0.349644\n",
      "[699]\tvalidation_0-mae:0.349639\n",
      "[700]\tvalidation_0-mae:0.349611\n",
      "[701]\tvalidation_0-mae:0.3496\n",
      "[702]\tvalidation_0-mae:0.349595\n",
      "[703]\tvalidation_0-mae:0.349585\n",
      "[704]\tvalidation_0-mae:0.349565\n",
      "[705]\tvalidation_0-mae:0.349541\n",
      "[706]\tvalidation_0-mae:0.349526\n",
      "[707]\tvalidation_0-mae:0.349507\n",
      "[708]\tvalidation_0-mae:0.349466\n",
      "[709]\tvalidation_0-mae:0.349435\n",
      "[710]\tvalidation_0-mae:0.349416\n",
      "[711]\tvalidation_0-mae:0.349399\n",
      "[712]\tvalidation_0-mae:0.349373\n",
      "[713]\tvalidation_0-mae:0.34935\n",
      "[714]\tvalidation_0-mae:0.349313\n",
      "[715]\tvalidation_0-mae:0.349264\n",
      "[716]\tvalidation_0-mae:0.349238\n",
      "[717]\tvalidation_0-mae:0.349207\n",
      "[718]\tvalidation_0-mae:0.349212\n",
      "[719]\tvalidation_0-mae:0.349191\n",
      "[720]\tvalidation_0-mae:0.34917\n",
      "[721]\tvalidation_0-mae:0.349163\n",
      "[722]\tvalidation_0-mae:0.349152\n",
      "[723]\tvalidation_0-mae:0.349129\n",
      "[724]\tvalidation_0-mae:0.349105\n",
      "[725]\tvalidation_0-mae:0.349089\n",
      "[726]\tvalidation_0-mae:0.349063\n",
      "[727]\tvalidation_0-mae:0.349037\n",
      "[728]\tvalidation_0-mae:0.349053\n",
      "[729]\tvalidation_0-mae:0.349018\n",
      "[730]\tvalidation_0-mae:0.348999\n",
      "[731]\tvalidation_0-mae:0.348988\n",
      "[732]\tvalidation_0-mae:0.348962\n",
      "[733]\tvalidation_0-mae:0.348935\n",
      "[734]\tvalidation_0-mae:0.348914\n",
      "[735]\tvalidation_0-mae:0.348862\n",
      "[736]\tvalidation_0-mae:0.348862\n",
      "[737]\tvalidation_0-mae:0.348859\n",
      "[738]\tvalidation_0-mae:0.348833\n",
      "[739]\tvalidation_0-mae:0.348821\n",
      "[740]\tvalidation_0-mae:0.348808\n",
      "[741]\tvalidation_0-mae:0.348765\n",
      "[742]\tvalidation_0-mae:0.34876\n",
      "[743]\tvalidation_0-mae:0.348731\n",
      "[744]\tvalidation_0-mae:0.348712\n",
      "[745]\tvalidation_0-mae:0.348711\n",
      "[746]\tvalidation_0-mae:0.3487\n",
      "[747]\tvalidation_0-mae:0.348663\n",
      "[748]\tvalidation_0-mae:0.34864\n",
      "[749]\tvalidation_0-mae:0.348611\n",
      "[750]\tvalidation_0-mae:0.348599\n",
      "[751]\tvalidation_0-mae:0.348578\n",
      "[752]\tvalidation_0-mae:0.348549\n",
      "[753]\tvalidation_0-mae:0.34852\n",
      "[754]\tvalidation_0-mae:0.348524\n",
      "[755]\tvalidation_0-mae:0.348508\n",
      "[756]\tvalidation_0-mae:0.348479\n",
      "[757]\tvalidation_0-mae:0.34846\n",
      "[758]\tvalidation_0-mae:0.348432\n",
      "[759]\tvalidation_0-mae:0.348424\n",
      "[760]\tvalidation_0-mae:0.348411\n",
      "[761]\tvalidation_0-mae:0.348388\n",
      "[762]\tvalidation_0-mae:0.348383\n",
      "[763]\tvalidation_0-mae:0.348371\n",
      "[764]\tvalidation_0-mae:0.348354\n",
      "[765]\tvalidation_0-mae:0.348337\n",
      "[766]\tvalidation_0-mae:0.348312\n",
      "[767]\tvalidation_0-mae:0.348309\n",
      "[768]\tvalidation_0-mae:0.3483\n",
      "[769]\tvalidation_0-mae:0.34828\n",
      "[770]\tvalidation_0-mae:0.34828\n",
      "[771]\tvalidation_0-mae:0.348257\n",
      "[772]\tvalidation_0-mae:0.348244\n",
      "[773]\tvalidation_0-mae:0.348224\n",
      "[774]\tvalidation_0-mae:0.348191\n",
      "[775]\tvalidation_0-mae:0.34817\n",
      "[776]\tvalidation_0-mae:0.348154\n",
      "[777]\tvalidation_0-mae:0.348121\n",
      "[778]\tvalidation_0-mae:0.348086\n",
      "[779]\tvalidation_0-mae:0.348058\n",
      "[780]\tvalidation_0-mae:0.348003\n",
      "[781]\tvalidation_0-mae:0.347983\n",
      "[782]\tvalidation_0-mae:0.347964\n",
      "[783]\tvalidation_0-mae:0.347943\n",
      "[784]\tvalidation_0-mae:0.347912\n",
      "[785]\tvalidation_0-mae:0.347889\n",
      "[786]\tvalidation_0-mae:0.347869\n",
      "[787]\tvalidation_0-mae:0.347855\n",
      "[788]\tvalidation_0-mae:0.347827\n",
      "[789]\tvalidation_0-mae:0.3478\n",
      "[790]\tvalidation_0-mae:0.347772\n",
      "[791]\tvalidation_0-mae:0.347751\n",
      "[792]\tvalidation_0-mae:0.347722\n",
      "[793]\tvalidation_0-mae:0.347713\n",
      "[794]\tvalidation_0-mae:0.347701\n",
      "[795]\tvalidation_0-mae:0.347692\n",
      "[796]\tvalidation_0-mae:0.347661\n",
      "[797]\tvalidation_0-mae:0.347634\n",
      "[798]\tvalidation_0-mae:0.347623\n",
      "[799]\tvalidation_0-mae:0.347611\n",
      "[800]\tvalidation_0-mae:0.347592\n",
      "[801]\tvalidation_0-mae:0.347567\n",
      "[802]\tvalidation_0-mae:0.347558\n",
      "[803]\tvalidation_0-mae:0.347548\n",
      "[804]\tvalidation_0-mae:0.347531\n",
      "[805]\tvalidation_0-mae:0.347517\n",
      "[806]\tvalidation_0-mae:0.347492\n",
      "[807]\tvalidation_0-mae:0.347478\n",
      "[808]\tvalidation_0-mae:0.347472\n",
      "[809]\tvalidation_0-mae:0.347456\n",
      "[810]\tvalidation_0-mae:0.34744\n",
      "[811]\tvalidation_0-mae:0.347423\n",
      "[812]\tvalidation_0-mae:0.347419\n",
      "[813]\tvalidation_0-mae:0.347413\n",
      "[814]\tvalidation_0-mae:0.347407\n",
      "[815]\tvalidation_0-mae:0.347394\n",
      "[816]\tvalidation_0-mae:0.347394\n",
      "[817]\tvalidation_0-mae:0.347381\n",
      "[818]\tvalidation_0-mae:0.347373\n",
      "[819]\tvalidation_0-mae:0.347358\n",
      "[820]\tvalidation_0-mae:0.347316\n",
      "[821]\tvalidation_0-mae:0.347305\n",
      "[822]\tvalidation_0-mae:0.347301\n",
      "[823]\tvalidation_0-mae:0.347281\n",
      "[824]\tvalidation_0-mae:0.347264\n",
      "[825]\tvalidation_0-mae:0.347269\n",
      "[826]\tvalidation_0-mae:0.347252\n",
      "[827]\tvalidation_0-mae:0.347237\n",
      "[828]\tvalidation_0-mae:0.347235\n",
      "[829]\tvalidation_0-mae:0.347214\n",
      "[830]\tvalidation_0-mae:0.347203\n",
      "[831]\tvalidation_0-mae:0.347196\n",
      "[832]\tvalidation_0-mae:0.347186\n",
      "[833]\tvalidation_0-mae:0.347188\n",
      "[834]\tvalidation_0-mae:0.347181\n",
      "[835]\tvalidation_0-mae:0.347173\n",
      "[836]\tvalidation_0-mae:0.347154\n",
      "[837]\tvalidation_0-mae:0.347165\n",
      "[838]\tvalidation_0-mae:0.347154\n",
      "[839]\tvalidation_0-mae:0.347148\n",
      "[840]\tvalidation_0-mae:0.347131\n",
      "[841]\tvalidation_0-mae:0.34711\n",
      "[842]\tvalidation_0-mae:0.347104\n",
      "[843]\tvalidation_0-mae:0.34709\n",
      "[844]\tvalidation_0-mae:0.347076\n",
      "[845]\tvalidation_0-mae:0.347064\n",
      "[846]\tvalidation_0-mae:0.347042\n",
      "[847]\tvalidation_0-mae:0.347027\n",
      "[848]\tvalidation_0-mae:0.347011\n",
      "[849]\tvalidation_0-mae:0.346996\n",
      "[850]\tvalidation_0-mae:0.346962\n",
      "[851]\tvalidation_0-mae:0.346945\n",
      "[852]\tvalidation_0-mae:0.346931\n",
      "[853]\tvalidation_0-mae:0.346918\n",
      "[854]\tvalidation_0-mae:0.346907\n",
      "[855]\tvalidation_0-mae:0.346896\n",
      "[856]\tvalidation_0-mae:0.346875\n",
      "[857]\tvalidation_0-mae:0.34686\n",
      "[858]\tvalidation_0-mae:0.346838\n",
      "[859]\tvalidation_0-mae:0.346827\n",
      "[860]\tvalidation_0-mae:0.346804\n",
      "[861]\tvalidation_0-mae:0.346803\n",
      "[862]\tvalidation_0-mae:0.346792\n",
      "[863]\tvalidation_0-mae:0.346782\n",
      "[864]\tvalidation_0-mae:0.346779\n",
      "[865]\tvalidation_0-mae:0.346778\n",
      "[866]\tvalidation_0-mae:0.346774\n",
      "[867]\tvalidation_0-mae:0.346753\n",
      "[868]\tvalidation_0-mae:0.346747\n",
      "[869]\tvalidation_0-mae:0.346723\n",
      "[870]\tvalidation_0-mae:0.346706\n",
      "[871]\tvalidation_0-mae:0.346696\n",
      "[872]\tvalidation_0-mae:0.346692\n",
      "[873]\tvalidation_0-mae:0.346674\n",
      "[874]\tvalidation_0-mae:0.346674\n",
      "[875]\tvalidation_0-mae:0.346647\n",
      "[876]\tvalidation_0-mae:0.346644\n",
      "[877]\tvalidation_0-mae:0.346642\n",
      "[878]\tvalidation_0-mae:0.346625\n",
      "[879]\tvalidation_0-mae:0.346625\n",
      "[880]\tvalidation_0-mae:0.346623\n",
      "[881]\tvalidation_0-mae:0.346604\n",
      "[882]\tvalidation_0-mae:0.346605\n",
      "[883]\tvalidation_0-mae:0.346579\n",
      "[884]\tvalidation_0-mae:0.346563\n",
      "[885]\tvalidation_0-mae:0.346541\n",
      "[886]\tvalidation_0-mae:0.346519\n",
      "[887]\tvalidation_0-mae:0.346501\n",
      "[888]\tvalidation_0-mae:0.346474\n",
      "[889]\tvalidation_0-mae:0.346468\n",
      "[890]\tvalidation_0-mae:0.346467\n",
      "[891]\tvalidation_0-mae:0.346423\n",
      "[892]\tvalidation_0-mae:0.346423\n",
      "[893]\tvalidation_0-mae:0.346414\n",
      "[894]\tvalidation_0-mae:0.346395\n",
      "[895]\tvalidation_0-mae:0.346389\n",
      "[896]\tvalidation_0-mae:0.346391\n",
      "[897]\tvalidation_0-mae:0.346384\n",
      "[898]\tvalidation_0-mae:0.346368\n",
      "[899]\tvalidation_0-mae:0.346363\n",
      "[900]\tvalidation_0-mae:0.346341\n",
      "[901]\tvalidation_0-mae:0.346327\n",
      "[902]\tvalidation_0-mae:0.346317\n",
      "[903]\tvalidation_0-mae:0.346301\n",
      "[904]\tvalidation_0-mae:0.346303\n",
      "[905]\tvalidation_0-mae:0.34629\n",
      "[906]\tvalidation_0-mae:0.346273\n",
      "[907]\tvalidation_0-mae:0.346253\n",
      "[908]\tvalidation_0-mae:0.346248\n",
      "[909]\tvalidation_0-mae:0.346244\n",
      "[910]\tvalidation_0-mae:0.346222\n",
      "[911]\tvalidation_0-mae:0.346206\n",
      "[912]\tvalidation_0-mae:0.346198\n",
      "[913]\tvalidation_0-mae:0.346206\n",
      "[914]\tvalidation_0-mae:0.346198\n",
      "[915]\tvalidation_0-mae:0.346193\n",
      "[916]\tvalidation_0-mae:0.346164\n",
      "[917]\tvalidation_0-mae:0.346159\n",
      "[918]\tvalidation_0-mae:0.346148\n",
      "[919]\tvalidation_0-mae:0.346129\n",
      "[920]\tvalidation_0-mae:0.346116\n",
      "[921]\tvalidation_0-mae:0.3461\n",
      "[922]\tvalidation_0-mae:0.346101\n",
      "[923]\tvalidation_0-mae:0.346094\n",
      "[924]\tvalidation_0-mae:0.346088\n",
      "[925]\tvalidation_0-mae:0.34608\n",
      "[926]\tvalidation_0-mae:0.346078\n",
      "[927]\tvalidation_0-mae:0.346064\n",
      "[928]\tvalidation_0-mae:0.346064\n",
      "[929]\tvalidation_0-mae:0.346057\n",
      "[930]\tvalidation_0-mae:0.346068\n",
      "[931]\tvalidation_0-mae:0.346035\n",
      "[932]\tvalidation_0-mae:0.346013\n",
      "[933]\tvalidation_0-mae:0.346001\n",
      "[934]\tvalidation_0-mae:0.346\n",
      "[935]\tvalidation_0-mae:0.345981\n",
      "[936]\tvalidation_0-mae:0.345976\n",
      "[937]\tvalidation_0-mae:0.345976\n",
      "[938]\tvalidation_0-mae:0.345974\n",
      "[939]\tvalidation_0-mae:0.345952\n",
      "[940]\tvalidation_0-mae:0.345928\n",
      "[941]\tvalidation_0-mae:0.34591\n",
      "[942]\tvalidation_0-mae:0.345906\n",
      "[943]\tvalidation_0-mae:0.345901\n",
      "[944]\tvalidation_0-mae:0.345897\n",
      "[945]\tvalidation_0-mae:0.345895\n",
      "[946]\tvalidation_0-mae:0.34589\n",
      "[947]\tvalidation_0-mae:0.345882\n",
      "[948]\tvalidation_0-mae:0.345879\n",
      "[949]\tvalidation_0-mae:0.345875\n",
      "[950]\tvalidation_0-mae:0.345876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[951]\tvalidation_0-mae:0.345857\n",
      "[952]\tvalidation_0-mae:0.345846\n",
      "[953]\tvalidation_0-mae:0.345839\n",
      "[954]\tvalidation_0-mae:0.345834\n",
      "[955]\tvalidation_0-mae:0.345818\n",
      "[956]\tvalidation_0-mae:0.345808\n",
      "[957]\tvalidation_0-mae:0.345789\n",
      "[958]\tvalidation_0-mae:0.345785\n",
      "[959]\tvalidation_0-mae:0.345782\n",
      "[960]\tvalidation_0-mae:0.345774\n",
      "[961]\tvalidation_0-mae:0.345778\n",
      "[962]\tvalidation_0-mae:0.345766\n",
      "[963]\tvalidation_0-mae:0.345763\n",
      "[964]\tvalidation_0-mae:0.345746\n",
      "[965]\tvalidation_0-mae:0.345744\n",
      "[966]\tvalidation_0-mae:0.345718\n",
      "[967]\tvalidation_0-mae:0.345698\n",
      "[968]\tvalidation_0-mae:0.345671\n",
      "[969]\tvalidation_0-mae:0.345669\n",
      "[970]\tvalidation_0-mae:0.34565\n",
      "[971]\tvalidation_0-mae:0.345645\n",
      "[972]\tvalidation_0-mae:0.345638\n",
      "[973]\tvalidation_0-mae:0.345629\n",
      "[974]\tvalidation_0-mae:0.345629\n",
      "[975]\tvalidation_0-mae:0.345618\n",
      "[976]\tvalidation_0-mae:0.345624\n",
      "[977]\tvalidation_0-mae:0.345614\n",
      "[978]\tvalidation_0-mae:0.345605\n",
      "[979]\tvalidation_0-mae:0.345604\n",
      "[980]\tvalidation_0-mae:0.345607\n",
      "[981]\tvalidation_0-mae:0.345596\n",
      "[982]\tvalidation_0-mae:0.345594\n",
      "[983]\tvalidation_0-mae:0.345586\n",
      "[984]\tvalidation_0-mae:0.34557\n",
      "[985]\tvalidation_0-mae:0.345564\n",
      "[986]\tvalidation_0-mae:0.345566\n",
      "[987]\tvalidation_0-mae:0.345545\n",
      "[988]\tvalidation_0-mae:0.345541\n",
      "[989]\tvalidation_0-mae:0.345533\n",
      "[990]\tvalidation_0-mae:0.345526\n",
      "[991]\tvalidation_0-mae:0.345528\n",
      "[992]\tvalidation_0-mae:0.345517\n",
      "[993]\tvalidation_0-mae:0.345503\n",
      "[994]\tvalidation_0-mae:0.345495\n",
      "[995]\tvalidation_0-mae:0.345484\n",
      "[996]\tvalidation_0-mae:0.345472\n",
      "[997]\tvalidation_0-mae:0.34546\n",
      "[998]\tvalidation_0-mae:0.345436\n",
      "[999]\tvalidation_0-mae:0.34542\n",
      "[1000]\tvalidation_0-mae:0.345406\n",
      "[1001]\tvalidation_0-mae:0.345391\n",
      "[1002]\tvalidation_0-mae:0.345385\n",
      "[1003]\tvalidation_0-mae:0.345375\n",
      "[1004]\tvalidation_0-mae:0.345374\n",
      "[1005]\tvalidation_0-mae:0.345356\n",
      "[1006]\tvalidation_0-mae:0.34535\n",
      "[1007]\tvalidation_0-mae:0.345342\n",
      "[1008]\tvalidation_0-mae:0.34534\n",
      "[1009]\tvalidation_0-mae:0.345325\n",
      "[1010]\tvalidation_0-mae:0.345319\n",
      "[1011]\tvalidation_0-mae:0.345295\n",
      "[1012]\tvalidation_0-mae:0.34527\n",
      "[1013]\tvalidation_0-mae:0.345256\n",
      "[1014]\tvalidation_0-mae:0.345248\n",
      "[1015]\tvalidation_0-mae:0.345238\n",
      "[1016]\tvalidation_0-mae:0.345234\n",
      "[1017]\tvalidation_0-mae:0.345229\n",
      "[1018]\tvalidation_0-mae:0.345225\n",
      "[1019]\tvalidation_0-mae:0.345208\n",
      "[1020]\tvalidation_0-mae:0.345208\n",
      "[1021]\tvalidation_0-mae:0.345208\n",
      "[1022]\tvalidation_0-mae:0.345193\n",
      "[1023]\tvalidation_0-mae:0.345185\n",
      "[1024]\tvalidation_0-mae:0.345171\n",
      "[1025]\tvalidation_0-mae:0.345152\n",
      "[1026]\tvalidation_0-mae:0.345146\n",
      "[1027]\tvalidation_0-mae:0.345149\n",
      "[1028]\tvalidation_0-mae:0.345139\n",
      "[1029]\tvalidation_0-mae:0.345119\n",
      "[1030]\tvalidation_0-mae:0.345121\n",
      "[1031]\tvalidation_0-mae:0.34511\n",
      "[1032]\tvalidation_0-mae:0.345108\n",
      "[1033]\tvalidation_0-mae:0.345109\n",
      "[1034]\tvalidation_0-mae:0.345089\n",
      "[1035]\tvalidation_0-mae:0.345085\n",
      "[1036]\tvalidation_0-mae:0.345079\n",
      "[1037]\tvalidation_0-mae:0.345072\n",
      "[1038]\tvalidation_0-mae:0.345049\n",
      "[1039]\tvalidation_0-mae:0.345042\n",
      "[1040]\tvalidation_0-mae:0.34502\n",
      "[1041]\tvalidation_0-mae:0.345017\n",
      "[1042]\tvalidation_0-mae:0.345001\n",
      "[1043]\tvalidation_0-mae:0.344996\n",
      "[1044]\tvalidation_0-mae:0.344992\n",
      "[1045]\tvalidation_0-mae:0.344987\n",
      "[1046]\tvalidation_0-mae:0.344976\n",
      "[1047]\tvalidation_0-mae:0.344962\n",
      "[1048]\tvalidation_0-mae:0.344951\n",
      "[1049]\tvalidation_0-mae:0.344954\n",
      "[1050]\tvalidation_0-mae:0.344947\n",
      "[1051]\tvalidation_0-mae:0.344934\n",
      "[1052]\tvalidation_0-mae:0.344931\n",
      "[1053]\tvalidation_0-mae:0.344902\n",
      "[1054]\tvalidation_0-mae:0.3449\n",
      "[1055]\tvalidation_0-mae:0.344893\n",
      "[1056]\tvalidation_0-mae:0.344878\n",
      "[1057]\tvalidation_0-mae:0.344875\n",
      "[1058]\tvalidation_0-mae:0.344861\n",
      "[1059]\tvalidation_0-mae:0.344859\n",
      "[1060]\tvalidation_0-mae:0.344852\n",
      "[1061]\tvalidation_0-mae:0.344854\n",
      "[1062]\tvalidation_0-mae:0.344855\n",
      "[1063]\tvalidation_0-mae:0.344848\n",
      "[1064]\tvalidation_0-mae:0.344842\n",
      "[1065]\tvalidation_0-mae:0.344835\n",
      "[1066]\tvalidation_0-mae:0.34483\n",
      "[1067]\tvalidation_0-mae:0.344827\n",
      "[1068]\tvalidation_0-mae:0.344826\n",
      "[1069]\tvalidation_0-mae:0.344823\n",
      "[1070]\tvalidation_0-mae:0.344803\n",
      "[1071]\tvalidation_0-mae:0.344788\n",
      "[1072]\tvalidation_0-mae:0.344781\n",
      "[1073]\tvalidation_0-mae:0.344774\n",
      "[1074]\tvalidation_0-mae:0.344759\n",
      "[1075]\tvalidation_0-mae:0.344752\n",
      "[1076]\tvalidation_0-mae:0.344726\n",
      "[1077]\tvalidation_0-mae:0.34472\n",
      "[1078]\tvalidation_0-mae:0.344711\n",
      "[1079]\tvalidation_0-mae:0.344713\n",
      "[1080]\tvalidation_0-mae:0.344704\n",
      "[1081]\tvalidation_0-mae:0.344697\n",
      "[1082]\tvalidation_0-mae:0.344687\n",
      "[1083]\tvalidation_0-mae:0.344675\n",
      "[1084]\tvalidation_0-mae:0.344672\n",
      "[1085]\tvalidation_0-mae:0.344667\n",
      "[1086]\tvalidation_0-mae:0.344657\n",
      "[1087]\tvalidation_0-mae:0.344649\n",
      "[1088]\tvalidation_0-mae:0.344643\n",
      "[1089]\tvalidation_0-mae:0.344641\n",
      "[1090]\tvalidation_0-mae:0.344631\n",
      "[1091]\tvalidation_0-mae:0.344618\n",
      "[1092]\tvalidation_0-mae:0.344617\n",
      "[1093]\tvalidation_0-mae:0.344606\n",
      "[1094]\tvalidation_0-mae:0.34461\n",
      "[1095]\tvalidation_0-mae:0.344603\n",
      "[1096]\tvalidation_0-mae:0.344622\n",
      "[1097]\tvalidation_0-mae:0.344611\n",
      "[1098]\tvalidation_0-mae:0.344599\n",
      "[1099]\tvalidation_0-mae:0.344591\n",
      "[1100]\tvalidation_0-mae:0.344582\n",
      "[1101]\tvalidation_0-mae:0.34457\n",
      "[1102]\tvalidation_0-mae:0.344563\n",
      "[1103]\tvalidation_0-mae:0.344556\n",
      "[1104]\tvalidation_0-mae:0.344551\n",
      "[1105]\tvalidation_0-mae:0.344548\n",
      "[1106]\tvalidation_0-mae:0.344541\n",
      "[1107]\tvalidation_0-mae:0.344537\n",
      "[1108]\tvalidation_0-mae:0.344529\n",
      "[1109]\tvalidation_0-mae:0.344518\n",
      "[1110]\tvalidation_0-mae:0.344507\n",
      "[1111]\tvalidation_0-mae:0.344507\n",
      "[1112]\tvalidation_0-mae:0.344508\n",
      "[1113]\tvalidation_0-mae:0.3445\n",
      "[1114]\tvalidation_0-mae:0.344489\n",
      "[1115]\tvalidation_0-mae:0.344478\n",
      "[1116]\tvalidation_0-mae:0.344477\n",
      "[1117]\tvalidation_0-mae:0.344479\n",
      "[1118]\tvalidation_0-mae:0.344475\n",
      "[1119]\tvalidation_0-mae:0.34446\n",
      "[1120]\tvalidation_0-mae:0.344461\n",
      "[1121]\tvalidation_0-mae:0.344461\n",
      "[1122]\tvalidation_0-mae:0.344454\n",
      "[1123]\tvalidation_0-mae:0.344449\n",
      "[1124]\tvalidation_0-mae:0.344447\n",
      "[1125]\tvalidation_0-mae:0.344433\n",
      "[1126]\tvalidation_0-mae:0.344419\n",
      "[1127]\tvalidation_0-mae:0.344413\n",
      "[1128]\tvalidation_0-mae:0.344415\n",
      "[1129]\tvalidation_0-mae:0.344411\n",
      "[1130]\tvalidation_0-mae:0.344415\n",
      "[1131]\tvalidation_0-mae:0.344405\n",
      "[1132]\tvalidation_0-mae:0.344396\n",
      "[1133]\tvalidation_0-mae:0.344396\n",
      "[1134]\tvalidation_0-mae:0.344391\n",
      "[1135]\tvalidation_0-mae:0.344383\n",
      "[1136]\tvalidation_0-mae:0.344378\n",
      "[1137]\tvalidation_0-mae:0.344365\n",
      "[1138]\tvalidation_0-mae:0.344359\n",
      "[1139]\tvalidation_0-mae:0.344359\n",
      "[1140]\tvalidation_0-mae:0.344345\n",
      "[1141]\tvalidation_0-mae:0.344335\n",
      "[1142]\tvalidation_0-mae:0.344329\n",
      "[1143]\tvalidation_0-mae:0.344321\n",
      "[1144]\tvalidation_0-mae:0.344312\n",
      "[1145]\tvalidation_0-mae:0.34431\n",
      "[1146]\tvalidation_0-mae:0.344297\n",
      "[1147]\tvalidation_0-mae:0.344287\n",
      "[1148]\tvalidation_0-mae:0.344292\n",
      "[1149]\tvalidation_0-mae:0.344281\n",
      "[1150]\tvalidation_0-mae:0.34427\n",
      "[1151]\tvalidation_0-mae:0.344265\n",
      "[1152]\tvalidation_0-mae:0.344257\n",
      "[1153]\tvalidation_0-mae:0.344252\n",
      "[1154]\tvalidation_0-mae:0.344247\n",
      "[1155]\tvalidation_0-mae:0.344234\n",
      "[1156]\tvalidation_0-mae:0.34422\n",
      "[1157]\tvalidation_0-mae:0.344209\n",
      "[1158]\tvalidation_0-mae:0.344205\n",
      "[1159]\tvalidation_0-mae:0.344198\n",
      "[1160]\tvalidation_0-mae:0.344191\n",
      "[1161]\tvalidation_0-mae:0.344195\n",
      "[1162]\tvalidation_0-mae:0.344191\n",
      "[1163]\tvalidation_0-mae:0.344184\n",
      "[1164]\tvalidation_0-mae:0.344182\n",
      "[1165]\tvalidation_0-mae:0.344169\n",
      "[1166]\tvalidation_0-mae:0.344164\n",
      "[1167]\tvalidation_0-mae:0.344161\n",
      "[1168]\tvalidation_0-mae:0.344141\n",
      "[1169]\tvalidation_0-mae:0.34414\n",
      "[1170]\tvalidation_0-mae:0.344134\n",
      "[1171]\tvalidation_0-mae:0.344124\n",
      "[1172]\tvalidation_0-mae:0.344123\n",
      "[1173]\tvalidation_0-mae:0.344124\n",
      "[1174]\tvalidation_0-mae:0.344109\n",
      "[1175]\tvalidation_0-mae:0.3441\n",
      "[1176]\tvalidation_0-mae:0.344087\n",
      "[1177]\tvalidation_0-mae:0.344083\n",
      "[1178]\tvalidation_0-mae:0.344083\n",
      "[1179]\tvalidation_0-mae:0.344074\n",
      "[1180]\tvalidation_0-mae:0.344071\n",
      "[1181]\tvalidation_0-mae:0.344068\n",
      "[1182]\tvalidation_0-mae:0.344074\n",
      "[1183]\tvalidation_0-mae:0.344073\n",
      "[1184]\tvalidation_0-mae:0.344064\n",
      "[1185]\tvalidation_0-mae:0.344047\n",
      "[1186]\tvalidation_0-mae:0.344044\n",
      "[1187]\tvalidation_0-mae:0.344033\n",
      "[1188]\tvalidation_0-mae:0.344027\n",
      "[1189]\tvalidation_0-mae:0.344022\n",
      "[1190]\tvalidation_0-mae:0.344003\n",
      "[1191]\tvalidation_0-mae:0.343986\n",
      "[1192]\tvalidation_0-mae:0.343987\n",
      "[1193]\tvalidation_0-mae:0.343979\n",
      "[1194]\tvalidation_0-mae:0.343975\n",
      "[1195]\tvalidation_0-mae:0.34397\n",
      "[1196]\tvalidation_0-mae:0.343966\n",
      "[1197]\tvalidation_0-mae:0.343954\n",
      "[1198]\tvalidation_0-mae:0.343951\n",
      "[1199]\tvalidation_0-mae:0.343937\n",
      "[1200]\tvalidation_0-mae:0.343922\n",
      "[1201]\tvalidation_0-mae:0.34392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1202]\tvalidation_0-mae:0.343906\n",
      "[1203]\tvalidation_0-mae:0.343905\n",
      "[1204]\tvalidation_0-mae:0.343899\n",
      "[1205]\tvalidation_0-mae:0.34389\n",
      "[1206]\tvalidation_0-mae:0.343888\n",
      "[1207]\tvalidation_0-mae:0.343878\n",
      "[1208]\tvalidation_0-mae:0.343873\n",
      "[1209]\tvalidation_0-mae:0.343867\n",
      "[1210]\tvalidation_0-mae:0.343864\n",
      "[1211]\tvalidation_0-mae:0.343864\n",
      "[1212]\tvalidation_0-mae:0.343857\n",
      "[1213]\tvalidation_0-mae:0.343855\n",
      "[1214]\tvalidation_0-mae:0.343841\n",
      "[1215]\tvalidation_0-mae:0.343839\n",
      "[1216]\tvalidation_0-mae:0.343836\n",
      "[1217]\tvalidation_0-mae:0.343837\n",
      "[1218]\tvalidation_0-mae:0.343825\n",
      "[1219]\tvalidation_0-mae:0.34382\n",
      "[1220]\tvalidation_0-mae:0.343802\n",
      "[1221]\tvalidation_0-mae:0.343796\n",
      "[1222]\tvalidation_0-mae:0.343788\n",
      "[1223]\tvalidation_0-mae:0.343793\n",
      "[1224]\tvalidation_0-mae:0.343789\n",
      "[1225]\tvalidation_0-mae:0.34378\n",
      "[1226]\tvalidation_0-mae:0.34377\n",
      "[1227]\tvalidation_0-mae:0.34376\n",
      "[1228]\tvalidation_0-mae:0.343754\n",
      "[1229]\tvalidation_0-mae:0.343754\n",
      "[1230]\tvalidation_0-mae:0.343757\n",
      "[1231]\tvalidation_0-mae:0.34375\n",
      "[1232]\tvalidation_0-mae:0.343742\n",
      "[1233]\tvalidation_0-mae:0.343743\n",
      "[1234]\tvalidation_0-mae:0.343739\n",
      "[1235]\tvalidation_0-mae:0.343733\n",
      "[1236]\tvalidation_0-mae:0.343726\n",
      "[1237]\tvalidation_0-mae:0.343718\n",
      "[1238]\tvalidation_0-mae:0.343715\n",
      "[1239]\tvalidation_0-mae:0.343715\n",
      "[1240]\tvalidation_0-mae:0.343713\n",
      "[1241]\tvalidation_0-mae:0.34371\n",
      "[1242]\tvalidation_0-mae:0.34371\n",
      "[1243]\tvalidation_0-mae:0.343702\n",
      "[1244]\tvalidation_0-mae:0.343699\n",
      "[1245]\tvalidation_0-mae:0.343701\n",
      "[1246]\tvalidation_0-mae:0.343695\n",
      "[1247]\tvalidation_0-mae:0.343687\n",
      "[1248]\tvalidation_0-mae:0.343678\n",
      "[1249]\tvalidation_0-mae:0.343675\n",
      "[1250]\tvalidation_0-mae:0.343662\n",
      "[1251]\tvalidation_0-mae:0.343651\n",
      "[1252]\tvalidation_0-mae:0.343641\n",
      "[1253]\tvalidation_0-mae:0.34364\n",
      "[1254]\tvalidation_0-mae:0.343634\n",
      "[1255]\tvalidation_0-mae:0.343621\n",
      "[1256]\tvalidation_0-mae:0.343619\n",
      "[1257]\tvalidation_0-mae:0.343605\n",
      "[1258]\tvalidation_0-mae:0.3436\n",
      "[1259]\tvalidation_0-mae:0.343593\n",
      "[1260]\tvalidation_0-mae:0.343593\n",
      "[1261]\tvalidation_0-mae:0.343586\n",
      "[1262]\tvalidation_0-mae:0.343585\n",
      "[1263]\tvalidation_0-mae:0.343574\n",
      "[1264]\tvalidation_0-mae:0.343569\n",
      "[1265]\tvalidation_0-mae:0.343564\n",
      "[1266]\tvalidation_0-mae:0.343557\n",
      "[1267]\tvalidation_0-mae:0.343546\n",
      "[1268]\tvalidation_0-mae:0.343537\n",
      "[1269]\tvalidation_0-mae:0.343531\n",
      "[1270]\tvalidation_0-mae:0.343533\n",
      "[1271]\tvalidation_0-mae:0.343523\n",
      "[1272]\tvalidation_0-mae:0.343514\n",
      "[1273]\tvalidation_0-mae:0.343507\n",
      "[1274]\tvalidation_0-mae:0.3435\n",
      "[1275]\tvalidation_0-mae:0.343495\n",
      "[1276]\tvalidation_0-mae:0.343489\n",
      "[1277]\tvalidation_0-mae:0.343483\n",
      "[1278]\tvalidation_0-mae:0.343487\n",
      "[1279]\tvalidation_0-mae:0.343481\n",
      "[1280]\tvalidation_0-mae:0.343482\n",
      "[1281]\tvalidation_0-mae:0.343483\n",
      "[1282]\tvalidation_0-mae:0.343477\n",
      "[1283]\tvalidation_0-mae:0.343477\n",
      "[1284]\tvalidation_0-mae:0.343467\n",
      "[1285]\tvalidation_0-mae:0.343471\n",
      "[1286]\tvalidation_0-mae:0.343463\n",
      "[1287]\tvalidation_0-mae:0.343463\n",
      "[1288]\tvalidation_0-mae:0.343455\n",
      "[1289]\tvalidation_0-mae:0.343459\n",
      "[1290]\tvalidation_0-mae:0.34345\n",
      "[1291]\tvalidation_0-mae:0.343449\n",
      "[1292]\tvalidation_0-mae:0.343448\n",
      "[1293]\tvalidation_0-mae:0.343443\n",
      "[1294]\tvalidation_0-mae:0.343439\n",
      "[1295]\tvalidation_0-mae:0.343432\n",
      "[1296]\tvalidation_0-mae:0.343431\n",
      "[1297]\tvalidation_0-mae:0.343432\n",
      "[1298]\tvalidation_0-mae:0.343431\n",
      "[1299]\tvalidation_0-mae:0.343429\n",
      "[1300]\tvalidation_0-mae:0.343423\n",
      "[1301]\tvalidation_0-mae:0.343423\n",
      "[1302]\tvalidation_0-mae:0.343421\n",
      "[1303]\tvalidation_0-mae:0.343415\n",
      "[1304]\tvalidation_0-mae:0.343416\n",
      "[1305]\tvalidation_0-mae:0.343415\n",
      "[1306]\tvalidation_0-mae:0.34341\n",
      "[1307]\tvalidation_0-mae:0.343409\n",
      "[1308]\tvalidation_0-mae:0.343392\n",
      "[1309]\tvalidation_0-mae:0.343381\n",
      "[1310]\tvalidation_0-mae:0.34337\n",
      "[1311]\tvalidation_0-mae:0.343359\n",
      "[1312]\tvalidation_0-mae:0.343347\n",
      "[1313]\tvalidation_0-mae:0.343343\n",
      "[1314]\tvalidation_0-mae:0.343338\n",
      "[1315]\tvalidation_0-mae:0.343338\n",
      "[1316]\tvalidation_0-mae:0.343336\n",
      "[1317]\tvalidation_0-mae:0.343329\n",
      "[1318]\tvalidation_0-mae:0.343329\n",
      "[1319]\tvalidation_0-mae:0.343324\n",
      "[1320]\tvalidation_0-mae:0.343314\n",
      "[1321]\tvalidation_0-mae:0.343307\n",
      "[1322]\tvalidation_0-mae:0.343305\n",
      "[1323]\tvalidation_0-mae:0.343298\n",
      "[1324]\tvalidation_0-mae:0.343293\n",
      "[1325]\tvalidation_0-mae:0.343291\n",
      "[1326]\tvalidation_0-mae:0.343291\n",
      "[1327]\tvalidation_0-mae:0.343285\n",
      "[1328]\tvalidation_0-mae:0.343277\n",
      "[1329]\tvalidation_0-mae:0.343273\n",
      "[1330]\tvalidation_0-mae:0.343271\n",
      "[1331]\tvalidation_0-mae:0.34327\n",
      "[1332]\tvalidation_0-mae:0.343265\n",
      "[1333]\tvalidation_0-mae:0.343266\n",
      "[1334]\tvalidation_0-mae:0.34326\n",
      "[1335]\tvalidation_0-mae:0.34326\n",
      "[1336]\tvalidation_0-mae:0.343252\n",
      "[1337]\tvalidation_0-mae:0.343252\n",
      "[1338]\tvalidation_0-mae:0.343248\n",
      "[1339]\tvalidation_0-mae:0.343241\n",
      "[1340]\tvalidation_0-mae:0.343242\n",
      "[1341]\tvalidation_0-mae:0.343235\n",
      "[1342]\tvalidation_0-mae:0.34323\n",
      "[1343]\tvalidation_0-mae:0.343223\n",
      "[1344]\tvalidation_0-mae:0.343218\n",
      "[1345]\tvalidation_0-mae:0.343212\n",
      "[1346]\tvalidation_0-mae:0.343209\n",
      "[1347]\tvalidation_0-mae:0.343207\n",
      "[1348]\tvalidation_0-mae:0.343199\n",
      "[1349]\tvalidation_0-mae:0.343194\n",
      "[1350]\tvalidation_0-mae:0.343192\n",
      "[1351]\tvalidation_0-mae:0.343191\n",
      "[1352]\tvalidation_0-mae:0.343189\n",
      "[1353]\tvalidation_0-mae:0.343187\n",
      "[1354]\tvalidation_0-mae:0.343188\n",
      "[1355]\tvalidation_0-mae:0.343186\n",
      "[1356]\tvalidation_0-mae:0.343187\n",
      "[1357]\tvalidation_0-mae:0.34318\n",
      "[1358]\tvalidation_0-mae:0.343175\n",
      "[1359]\tvalidation_0-mae:0.343166\n",
      "[1360]\tvalidation_0-mae:0.343159\n",
      "[1361]\tvalidation_0-mae:0.343163\n",
      "[1362]\tvalidation_0-mae:0.34316\n",
      "[1363]\tvalidation_0-mae:0.343151\n",
      "[1364]\tvalidation_0-mae:0.343144\n",
      "[1365]\tvalidation_0-mae:0.343139\n",
      "[1366]\tvalidation_0-mae:0.343143\n",
      "[1367]\tvalidation_0-mae:0.34314\n",
      "[1368]\tvalidation_0-mae:0.343145\n",
      "[1369]\tvalidation_0-mae:0.343144\n",
      "[1370]\tvalidation_0-mae:0.34314\n",
      "[1371]\tvalidation_0-mae:0.343144\n",
      "[1372]\tvalidation_0-mae:0.343135\n",
      "[1373]\tvalidation_0-mae:0.343131\n",
      "[1374]\tvalidation_0-mae:0.343132\n",
      "[1375]\tvalidation_0-mae:0.343124\n",
      "[1376]\tvalidation_0-mae:0.343121\n",
      "[1377]\tvalidation_0-mae:0.343117\n",
      "[1378]\tvalidation_0-mae:0.343112\n",
      "[1379]\tvalidation_0-mae:0.343102\n",
      "[1380]\tvalidation_0-mae:0.343095\n",
      "[1381]\tvalidation_0-mae:0.343094\n",
      "[1382]\tvalidation_0-mae:0.343085\n",
      "[1383]\tvalidation_0-mae:0.34308\n",
      "[1384]\tvalidation_0-mae:0.343079\n",
      "[1385]\tvalidation_0-mae:0.343076\n",
      "[1386]\tvalidation_0-mae:0.343073\n",
      "[1387]\tvalidation_0-mae:0.343069\n",
      "[1388]\tvalidation_0-mae:0.34307\n",
      "[1389]\tvalidation_0-mae:0.343069\n",
      "[1390]\tvalidation_0-mae:0.343063\n",
      "[1391]\tvalidation_0-mae:0.343056\n",
      "[1392]\tvalidation_0-mae:0.343053\n",
      "[1393]\tvalidation_0-mae:0.343044\n",
      "[1394]\tvalidation_0-mae:0.343044\n",
      "[1395]\tvalidation_0-mae:0.343041\n",
      "[1396]\tvalidation_0-mae:0.343037\n",
      "[1397]\tvalidation_0-mae:0.343034\n",
      "[1398]\tvalidation_0-mae:0.343027\n",
      "[1399]\tvalidation_0-mae:0.343021\n",
      "[1400]\tvalidation_0-mae:0.343011\n",
      "[1401]\tvalidation_0-mae:0.343002\n",
      "[1402]\tvalidation_0-mae:0.342996\n",
      "[1403]\tvalidation_0-mae:0.342992\n",
      "[1404]\tvalidation_0-mae:0.342981\n",
      "[1405]\tvalidation_0-mae:0.34298\n",
      "[1406]\tvalidation_0-mae:0.342978\n",
      "[1407]\tvalidation_0-mae:0.342977\n",
      "[1408]\tvalidation_0-mae:0.342979\n",
      "[1409]\tvalidation_0-mae:0.342981\n",
      "[1410]\tvalidation_0-mae:0.34298\n",
      "[1411]\tvalidation_0-mae:0.342973\n",
      "[1412]\tvalidation_0-mae:0.34297\n",
      "[1413]\tvalidation_0-mae:0.342968\n",
      "[1414]\tvalidation_0-mae:0.342965\n",
      "[1415]\tvalidation_0-mae:0.342962\n",
      "[1416]\tvalidation_0-mae:0.342957\n",
      "[1417]\tvalidation_0-mae:0.342955\n",
      "[1418]\tvalidation_0-mae:0.342955\n",
      "[1419]\tvalidation_0-mae:0.34295\n",
      "[1420]\tvalidation_0-mae:0.34295\n",
      "[1421]\tvalidation_0-mae:0.342933\n",
      "[1422]\tvalidation_0-mae:0.34293\n",
      "[1423]\tvalidation_0-mae:0.342925\n",
      "[1424]\tvalidation_0-mae:0.342923\n",
      "[1425]\tvalidation_0-mae:0.342922\n",
      "[1426]\tvalidation_0-mae:0.342919\n",
      "[1427]\tvalidation_0-mae:0.342917\n",
      "[1428]\tvalidation_0-mae:0.342912\n",
      "[1429]\tvalidation_0-mae:0.342911\n",
      "[1430]\tvalidation_0-mae:0.342905\n",
      "[1431]\tvalidation_0-mae:0.342897\n",
      "[1432]\tvalidation_0-mae:0.342897\n",
      "[1433]\tvalidation_0-mae:0.342891\n",
      "[1434]\tvalidation_0-mae:0.342883\n",
      "[1435]\tvalidation_0-mae:0.342868\n",
      "[1436]\tvalidation_0-mae:0.342866\n",
      "[1437]\tvalidation_0-mae:0.342864\n",
      "[1438]\tvalidation_0-mae:0.342867\n",
      "[1439]\tvalidation_0-mae:0.342867\n",
      "[1440]\tvalidation_0-mae:0.342864\n",
      "[1441]\tvalidation_0-mae:0.342862\n",
      "[1442]\tvalidation_0-mae:0.342852\n",
      "[1443]\tvalidation_0-mae:0.34285\n",
      "[1444]\tvalidation_0-mae:0.342848\n",
      "[1445]\tvalidation_0-mae:0.342838\n",
      "[1446]\tvalidation_0-mae:0.342838\n",
      "[1447]\tvalidation_0-mae:0.342833\n",
      "[1448]\tvalidation_0-mae:0.342829\n",
      "[1449]\tvalidation_0-mae:0.342826\n",
      "[1450]\tvalidation_0-mae:0.342828\n",
      "[1451]\tvalidation_0-mae:0.342825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1452]\tvalidation_0-mae:0.342825\n",
      "[1453]\tvalidation_0-mae:0.342825\n",
      "[1454]\tvalidation_0-mae:0.342825\n",
      "[1455]\tvalidation_0-mae:0.342825\n",
      "[1456]\tvalidation_0-mae:0.342825\n",
      "[1457]\tvalidation_0-mae:0.342825\n",
      "[1458]\tvalidation_0-mae:0.342825\n",
      "[1459]\tvalidation_0-mae:0.342825\n",
      "[1460]\tvalidation_0-mae:0.342825\n",
      "[1461]\tvalidation_0-mae:0.342825\n",
      "[1462]\tvalidation_0-mae:0.342825\n",
      "[1463]\tvalidation_0-mae:0.342825\n",
      "[1464]\tvalidation_0-mae:0.342825\n",
      "[1465]\tvalidation_0-mae:0.342825\n",
      "[1466]\tvalidation_0-mae:0.342825\n",
      "[1467]\tvalidation_0-mae:0.342825\n",
      "[1468]\tvalidation_0-mae:0.342825\n",
      "[1469]\tvalidation_0-mae:0.342825\n",
      "[1470]\tvalidation_0-mae:0.342825\n",
      "[1471]\tvalidation_0-mae:0.342825\n",
      "[1472]\tvalidation_0-mae:0.342825\n",
      "[1473]\tvalidation_0-mae:0.342825\n",
      "[1474]\tvalidation_0-mae:0.342825\n",
      "[1475]\tvalidation_0-mae:0.342825\n",
      "[1476]\tvalidation_0-mae:0.342825\n",
      "[1477]\tvalidation_0-mae:0.342825\n",
      "[1478]\tvalidation_0-mae:0.342825\n",
      "[1479]\tvalidation_0-mae:0.342825\n",
      "[1480]\tvalidation_0-mae:0.342825\n",
      "[1481]\tvalidation_0-mae:0.342825\n",
      "[1482]\tvalidation_0-mae:0.342825\n",
      "[1483]\tvalidation_0-mae:0.342825\n",
      "Stopping. Best iteration:\n",
      "[1451]\tvalidation_0-mae:0.342825\n",
      "\n",
      "validation score for 1JHN is -1.0705350873942576 with XGB\n",
      "\n",
      "error correlation: 0.08113892219419776, error difference -3.106954032421001\n",
      "accuracy_list with alphas for 1JHN:\n",
      " [-1.0705350873942576, -0.14550288731951877, 0.4648356460623258, 0.8499828679827219, 1.1298627831008066, 1.349168852324614, 1.529194869429107, 1.681946868320961, 1.8145221957080717, 1.931596347760396, 2.0364189450267434]\n",
      "blending with alpha = 0.0, final accuracy  for 1JHN = -1.0705350873942576\n",
      "predicting NN\n",
      "predicting XGB\n",
      "Predicting 1JHC out of ['1JHN', '1JHC', '2JHH', '2JHC', '2JHN', '3JHH', '3JHC', '3JHN'] at 08.13.2019_13.35.00\n",
      "Train on 638474 samples, validate on 70942 samples\n",
      "Epoch 1/2000\n",
      "354304/638474 [===============>..............] - ETA: 1:02 - loss: 90.9341"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-648db524cc3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_rd_nn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mnn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoupling_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mnn_val_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-960bdb929186>\u001b[0m in \u001b[0;36mnn_train\u001b[1;34m(coupling_type, train_X, train_y, validation_X, validation_y)\u001b[0m\n\u001b[0;32m     20\u001b[0m     history = nn_model.fit(x = train_X.values, y = train_y['scalar_coupling_constant'].values, \n\u001b[0;32m     21\u001b[0m                            \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'scalar_coupling_constant'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                            callbacks = [es, rlr, sv_mod], epochs = epoch_n, batch_size = batch_size, verbose = verbose)\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mins\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    529\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    529\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load = False\n",
    "\n",
    "df_test = pd.read_csv('../submissions/submission_best.csv')\n",
    "test_prediction = df_test['scalar_coupling_constant']\n",
    "df_test_full = pd.read_csv('../input/test.csv')\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "val_score = {\n",
    "    '1JHC': np.inf, '1JHN': np.inf, '2JHH': np.inf, '2JHC': np.inf, \n",
    "    '2JHN': np.inf, '3JHH': np.inf, '3JHC': np.inf, '3JHN': np.inf\n",
    "}\n",
    "\n",
    "coupling_types = ['1JHN', '1JHC', '2JHH', '2JHC', '2JHN', '3JHH', '3JHC', '3JHN']\n",
    "\n",
    "for coupling_type in coupling_types:\n",
    "    time = datetime.now(pytz.timezone('Europe/Oslo')).strftime('%m.%d.%Y_%H.%M.%S')\n",
    "    print(f'Predicting {coupling_type} out of {coupling_types} at {time}.')\n",
    "    \n",
    "    df_train, df_test = get_features(coupling_type)\n",
    "    train_X, train_y, validation_X, validation_y = split(df_train)\n",
    "    \n",
    "    ################################# NN #################################\n",
    "    \n",
    "    if load:         \n",
    "        model_name_rd_nn = f'models/nn/nn_model_{coupling_type}.hdf5'\n",
    "        nn_model = create_nn_model(validation_X.shape[1]) # vals\n",
    "        print(f'Loading weights from {model_name_rd_nn}.')\n",
    "        nn_model.load_weights(model_name_rd_nn)\n",
    "    else:\n",
    "        nn_model = nn_train(coupling_type, train_X, train_y, validation_X, validation_y)\n",
    "    \n",
    "    nn_val_predict = nn_model.predict(validation_X)\n",
    "\n",
    "    nn_accuracy = np.log(np.mean(np.abs(validation_y.values - nn_val_predict)))\n",
    "    print(f'Validation score for {coupling_type} is {nn_accuracy} with NN.\\n')\n",
    "    \n",
    "    ################################# XGB #################################\n",
    "\n",
    "    if load:\n",
    "        model_name_rd_xgb = f'..models/xgb/featurebook_{coupling_type}.joblib.dat'\n",
    "\n",
    "        xgb_model = xgb.XGBRegressor(base_score = 0.5, booster = 'gbtree', colsample_bylevel = 1,\n",
    "                                     colsample_bytree = 1, gamma = 0, importance_type = 'gain',\n",
    "                                     learning_rate = 0.1, max_delta_step = 0, max_depth = 9,\n",
    "                                     min_child_weight = 1, missing = None, n_estimators = 10000, n_jobs = -1,\n",
    "                                     nthread = None, objective = 'reg:squarederror', random_state = 101, reg_alpha = 2,\n",
    "                                     reg_lambda = 0.2, scale_pos_weight = 1, seed = None, silent = False, subsample = 1)\n",
    "\n",
    "        print(f'Loading weights from {model_name_rd_xgb}.')\n",
    "        #xgb_model.load_model(model_name_rd_xgb)\n",
    "        xgb_model= joblib.load(model_name_rd_xgb)\n",
    "    else:\n",
    "        xgb_model = xgb_train(coupling_type, train_X, train_y, validation_X, validation_y)   \n",
    "        \n",
    "    xgb_val_predict = xgb_model.predict(validation_X)\n",
    "    \n",
    "    memory_optimization([train_X, train_y, validation_X])\n",
    "    gc.collect()\n",
    "    \n",
    "    validation_y = validation_y['scalar_coupling_constant'].values\n",
    "\n",
    "    diff = validation_y - xgb_val_predict\n",
    "    xgb_accuracy = np.log(np.mean(np.abs(diff)))\n",
    "\n",
    "    print(f'Validation score for {coupling_type} is {xgb_accuracy} with XGB.\\n')\n",
    "     \n",
    "    ################################# BLEND #################################\n",
    "        \n",
    "    nn_val_predict = np.transpose(nn_val_predict[:,0])\n",
    "    log_accuracy_list = val_blending_list(nn_val_predict, xgb_val_predict, validation_y)\n",
    "    print(f'Accuracy_list with alphas for {coupling_type}:\\n {log_accuracy_list}')\n",
    "    \n",
    "    alpha_i = np.argmin(log_accuracy_list)\n",
    "    log_accuracy = log_accuracy_list[alpha_i]\n",
    "    val_score[coupling_type] = (log_accuracy)\n",
    "    alpha = alpha_i * 0.1\n",
    "    print(f'Blending with alpha = {alpha}, final accuracy for {coupling_type} = {log_accuracy}.') \n",
    "    \n",
    "    ################################# PREDICT #################################\n",
    "    \n",
    "    print('Predicting NN:')\n",
    "    nn_test_predict = nn_model.predict(df_test)\n",
    "    nn_test_predict_scaled = alpha * nn_test_predict\n",
    "    nn_test_predict_scaled = np.transpose(nn_test_predict_scaled[:, 0])\n",
    "    \n",
    "    print('Predicting XGB:')\n",
    "    xgb_test_predict = xgb_model.predict(df_test)\n",
    "    xgb_test_predict_scaled = (1 - alpha) * xgb_test_predict\n",
    "    \n",
    "    test_predict = nn_test_predict_scaled + xgb_test_predict_scaled\n",
    "    test_prediction[df_test_full['type'] == coupling_type] = test_predict\n",
    "\n",
    "    memory_optimization([df_test, nn_model, nn_val_predict, xgb_model, xgb_val_predict, nn_test_predict, \n",
    "                         nn_test_predict_scaled, xgb_test_predict, xgb_test_predict_scaled, test_predict])\n",
    "    gc.collect()\n",
    "    \n",
    "val_score_total = sum(val_score.values()) / len(val_score.keys())\n",
    "print(f'Total cv score is {val_score_total}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(predictions):\n",
    "    submit = pd.read_csv('submissions/submission_best.csv')  \n",
    "    submit['scalar_coupling_constant'] = predictions\n",
    "    submit.to_csv('submissions/submission_blended_full.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(test_prediction)\n",
    "\n",
    "time = datetime.now(pytz.timezone('Europe/Oslo')).strftime('%m.%d.%Y_%H.%M.%S')\n",
    "print(f'Notebook EoF reached at {time} and submission saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
