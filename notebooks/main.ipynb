{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pytz\n",
    "import operator\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import callbacks\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.losses import mean_absolute_error\n",
    "from tensorflow.python.keras.layers import Dense, Input, Activation\n",
    "from tensorflow.python.keras.layers import BatchNormalization, Add, Dropout\n",
    "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.python.keras.optimizers import Adam, Adadelta, SGD\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action = 'ignore', category = FutureWarning)\n",
    "warnings.filterwarnings(action = 'ignore', category = DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.generate_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'08.13.2019_15.03.14'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now(pytz.timezone('Europe/Oslo')).strftime('%m.%d.%Y_%H.%M.%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_optimization(dfs):\n",
    "    for df in dfs:\n",
    "        del df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df_train):\n",
    "    train_X, validation_X = train_test_split(df_train, test_size = 0.1, random_state = 0)\n",
    "\n",
    "    train_X = train_X.reset_index()\n",
    "    validation_X = validation_X.reset_index()\n",
    "\n",
    "    train_y = train_X['scalar_coupling_constant']\n",
    "    train_y = train_y.replace([np.inf, -np.inf], np.nan)\n",
    "    train_y = train_y.reset_index()\n",
    "    train_y = train_y.drop(['index'], axis = 1)\n",
    "    validation_y = validation_X['scalar_coupling_constant']\n",
    "    validation_y = validation_y.replace([np.inf, -np.inf], np.nan)\n",
    "    validation_y = validation_y.reset_index()\n",
    "    validation_y = validation_y.drop(['index'], axis = 1)\n",
    "\n",
    "    train_X = train_X.drop('scalar_coupling_constant', axis = 1)\n",
    "    validation_X = validation_X.drop('scalar_coupling_constant', axis = 1)\n",
    "    \n",
    "    train_X = train_X.drop(['index'], axis = 1)\n",
    "    validation_X = validation_X.drop(['index'], axis = 1)\n",
    "    \n",
    "    return train_X, train_y, validation_X, validation_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    # input layer\n",
    "    inp = Input(shape = (input_shape,))\n",
    "\n",
    "    # first hidden layer\n",
    "    x = Dense(256, kernel_initializer = 'he_normal')(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # second hidden layer\n",
    "    x = Dense(512, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # third hidden layer\n",
    "    x = Dense(1024, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # fourth hidden layer\n",
    "    x = Dense(512, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # fifth hidden layer\n",
    "    x = Dense(256, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # sixth hidden layer\n",
    "    x = Dense(128, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # seventh hidden layer\n",
    "    x = Dense(128, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # eight hidden layer\n",
    "    x = Dense(64, kernel_initializer = 'he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha = 0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # output layer scalar_coupling_constant\n",
    "    out = Dense(1, activation = 'linear')(x)   \n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(coupling_type, train_X, train_y, validation_X, validation_y):\n",
    "    epoch_n = 2000\n",
    "    verbose = 1\n",
    "    batch_size = 2048\n",
    "    model_name = f'../models/nn/coupling_model_{coupling_type}_NN.hdf5'\n",
    "    \n",
    "    nn_model = create_nn_model(train_X.shape[1])\n",
    "\n",
    "    nn_model.compile(loss = 'mae', optimizer = Adam())\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor = 'loss', min_delta = 0.00001, patience = 64,\n",
    "                                 verbose = verbose, mode = 'auto', restore_best_weights = True)\n",
    "    \n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 32,\n",
    "                                      min_lr = 1e-7, mode = 'auto', verbose = verbose)\n",
    "    \n",
    "    sv_mod = callbacks.ModelCheckpoint(model_name, monitor = 'val_loss', save_best_only = True,\n",
    "                                       save_weights_only = True, restore_best_weights = True)\n",
    "    \n",
    "    history = nn_model.fit(x = train_X.values, y = train_y['scalar_coupling_constant'].values, \n",
    "                           validation_data = (validation_X.values, validation_y['scalar_coupling_constant'].values),\n",
    "                           callbacks = [es, rlr, sv_mod], epochs = epoch_n, batch_size = batch_size, verbose = verbose)\n",
    "    \n",
    "    nn_model.save_weights(model_name)\n",
    "\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_train(coupling_type, train_X, train_y, validation_X, validation_y):\n",
    "    model_name_wrt = f'../models/xgb/coupling_model_{coupling_type}_XGB.hdf5'\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(base_score = 0.5, booster = 'gbtree', colsample_bylevel = 1,\n",
    "                                 colsample_bytree = 1, gamma = 0, importance_type = 'gain',\n",
    "                                 learning_rate = 0.1, max_delta_step = 0, max_depth = 9,\n",
    "                                 min_child_weight = 1, missing = None, n_estimators = 10000, n_jobs = -1,\n",
    "                                 nthread = None, objective = 'reg:squarederror', random_state = 101, reg_alpha = 2,\n",
    "                                 reg_lambda = 0.2, scale_pos_weight = 1, seed = None, silent = False, subsample = 1)\n",
    "\n",
    "    xgb_model.fit(train_X, train_y, eval_set = [(validation_X, validation_y)], eval_metric = 'mae', \n",
    "                  early_stopping_rounds = 32, verbose = True)   \n",
    "    \n",
    "    xgb_model.save_model(model_name_wrt)\n",
    "    #joblib.dump(xgb_model, model_name_wrt)\n",
    "    \n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance(xgb_model, train_X):\n",
    "    input_features = train_X.columns.values\n",
    "    feat_imp = xgb_model.feature_importances_\n",
    "    np.split(feat_imp, len(input_features))\n",
    "    \n",
    "    feat_imp_dict = {}\n",
    "    for i in range(0, len(input_features)):\n",
    "        feat_imp_dict[feat_imp[i]] = input_features[i]\n",
    "\n",
    "    sorted_feats = sorted(feat_imp_dict.items(), key = operator.itemgetter(0))\n",
    "    for i in range(len(sorted_feats) - 1, 0, -1):\n",
    "        print(sorted_feats[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Blending and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_blending_list(nn_val_predict, xgb_val_predict, validation_y):\n",
    "    nn_val_pred_error  = (validation_y - nn_val_predict)\n",
    "    xgb_val_pred_error = (validation_y - xgb_val_predict)\n",
    "    \n",
    "    val_error_corr = np.corrcoef(nn_val_pred_error, xgb_val_pred_error)\n",
    "    print(f'Error correlation: {val_error_corr[0][1]}. Error difference {xgb_accuracy - nn_accuracy}')\n",
    "    \n",
    "    log_accuracy = 0\n",
    "    log_accuracy_list = []\n",
    "    val_predict = np.array([])\n",
    "\n",
    "    for alpha in np.arange(0, 1.1, 0.1):\n",
    "        nn_val_predict_scaled = alpha * nn_val_predict\n",
    "        xgb_val_predict_scaled = (1 - alpha) * xgb_val_predict\n",
    "        val_predict = nn_val_predict_scaled + xgb_val_predict_scaled\n",
    "        log_accuracy = np.log(np.mean(np.abs(validation_y - val_predict)))\n",
    "        log_accuracy_list.append(log_accuracy)\n",
    "\n",
    "    return log_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-19e179235fc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[0mxgb_val_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb_model' is not defined"
     ]
    }
   ],
   "source": [
    "load = False\n",
    "\n",
    "df_test = pd.read_csv('../submissions/submission_best.csv')\n",
    "test_prediction = df_test['scalar_coupling_constant']\n",
    "df_test_full = pd.read_csv('../input/test.csv')\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "val_score = {\n",
    "    '1JHC': np.inf, '1JHN': np.inf, '2JHH': np.inf, '2JHC': np.inf, \n",
    "    '2JHN': np.inf, '3JHH': np.inf, '3JHC': np.inf, '3JHN': np.inf\n",
    "}\n",
    "\n",
    "coupling_types = ['1JHN', '1JHC', '2JHH', '2JHC', '2JHN', '3JHH', '3JHC', '3JHN']\n",
    "\n",
    "'''\n",
    "for coupling_type in coupling_types:\n",
    "    time = datetime.now(pytz.timezone('Europe/Oslo')).strftime('%m.%d.%Y_%H.%M.%S')\n",
    "    print(f'Predicting {coupling_type} out of {coupling_types} at {time}.')\n",
    "    \n",
    "    df_train, df_test = get_features(coupling_type)\n",
    "    train_X, train_y, validation_X, validation_y = split(df_train)\n",
    "    \n",
    "    ################################# NN #################################\n",
    "    \n",
    "    if load:         \n",
    "        model_name_rd_nn = f'models/nn/nn_model_{coupling_type}.hdf5'\n",
    "        nn_model = create_nn_model(validation_X.shape[1]) # vals\n",
    "        print(f'Loading weights from {model_name_rd_nn}.')\n",
    "        nn_model.load_weights(model_name_rd_nn)\n",
    "    else:\n",
    "        nn_model = nn_train(coupling_type, train_X, train_y, validation_X, validation_y)\n",
    "    \n",
    "    nn_val_predict = nn_model.predict(validation_X)\n",
    "\n",
    "    nn_accuracy = np.log(np.mean(np.abs(validation_y.values - nn_val_predict)))\n",
    "    print(f'Validation score for {coupling_type} is {nn_accuracy} with NN.\\n')\n",
    "    \n",
    "    ################################# XGB #################################\n",
    "\n",
    "    if load:\n",
    "        model_name_rd_xgb = f'..models/xgb/featurebook_{coupling_type}.joblib.dat'\n",
    "\n",
    "        xgb_model = xgb.XGBRegressor(base_score = 0.5, booster = 'gbtree', colsample_bylevel = 1,\n",
    "                                     colsample_bytree = 1, gamma = 0, importance_type = 'gain',\n",
    "                                     learning_rate = 0.1, max_delta_step = 0, max_depth = 9,\n",
    "                                     min_child_weight = 1, missing = None, n_estimators = 10000, n_jobs = -1,\n",
    "                                     nthread = None, objective = 'reg:squarederror', random_state = 101, reg_alpha = 2,\n",
    "                                     reg_lambda = 0.2, scale_pos_weight = 1, seed = None, silent = False, subsample = 1)\n",
    "\n",
    "        print(f'Loading weights from {model_name_rd_xgb}.')\n",
    "        #xgb_model.load_model(model_name_rd_xgb)\n",
    "        xgb_model= joblib.load(model_name_rd_xgb)\n",
    "    else:\n",
    "        xgb_model = xgb_train(coupling_type, train_X, train_y, validation_X, validation_y)   \n",
    "        \n",
    "    xgb_val_predict = xgb_model.predict(validation_X)\n",
    "    \n",
    "    memory_optimization([train_X, train_y, validation_X])\n",
    "    gc.collect()\n",
    "    \n",
    "    validation_y = validation_y['scalar_coupling_constant'].values\n",
    "\n",
    "    diff = validation_y - xgb_val_predict\n",
    "    xgb_accuracy = np.log(np.mean(np.abs(diff)))\n",
    "\n",
    "    print(f'Validation score for {coupling_type} is {xgb_accuracy} with XGB.\\n')\n",
    "     \n",
    "    ################################# BLEND #################################\n",
    "        \n",
    "    nn_val_predict = np.transpose(nn_val_predict[:,0])\n",
    "    log_accuracy_list = val_blending_list(nn_val_predict, xgb_val_predict, validation_y)\n",
    "    print(f'Accuracy_list with alphas for {coupling_type}:\\n {log_accuracy_list}')\n",
    "    \n",
    "    alpha_i = np.argmin(log_accuracy_list)\n",
    "    log_accuracy = log_accuracy_list[alpha_i]\n",
    "    val_score[coupling_type] = (log_accuracy)\n",
    "    alpha = alpha_i * 0.1\n",
    "    print(f'Blending with alpha = {alpha}, final accuracy for {coupling_type} = {log_accuracy}.') \n",
    "    \n",
    "    ################################# PREDICT #################################\n",
    "    \n",
    "    print('Predicting NN:')\n",
    "    nn_test_predict = nn_model.predict(df_test)\n",
    "    nn_test_predict_scaled = alpha * nn_test_predict\n",
    "    nn_test_predict_scaled = np.transpose(nn_test_predict_scaled[:, 0])\n",
    "    \n",
    "    print('Predicting XGB:')\n",
    "    xgb_test_predict = xgb_model.predict(df_test)\n",
    "    xgb_test_predict_scaled = (1 - alpha) * xgb_test_predict\n",
    "    \n",
    "    test_predict = nn_test_predict_scaled + xgb_test_predict_scaled\n",
    "    test_prediction[df_test_full['type'] == coupling_type] = test_predict\n",
    "\n",
    "    memory_optimization([df_test, nn_model, nn_val_predict, xgb_model, xgb_val_predict, nn_test_predict, \n",
    "                         nn_test_predict_scaled, xgb_test_predict, xgb_test_predict_scaled, test_predict])\n",
    "    gc.collect()\n",
    "    \n",
    "val_score_total = sum(val_score.values()) / len(val_score.keys())\n",
    "print(f'Total cv score is {val_score_total}.')\n",
    "'''\n",
    "\n",
    "for coupling_type in coupling_types:\n",
    "    df_train, df_test = get_features(coupling_type)\n",
    "    train_X, train_y, validation_X, validation_y = split(df_train)\n",
    "    \n",
    "    xgb_val_predict = xgb_model.predict(validation_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(predictions):\n",
    "    submit = pd.read_csv('submissions/submission_best.csv')  \n",
    "    submit['scalar_coupling_constant'] = predictions\n",
    "    submit.to_csv('submissions/submission_blended_full.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(test_prediction)\n",
    "\n",
    "time = datetime.now(pytz.timezone('Europe/Oslo')).strftime('%m.%d.%Y_%H.%M.%S')\n",
    "print(f'Notebook EoF reached at {time} and submission saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
